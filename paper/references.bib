% Классические и основополагающие работы

@article{vapnik1971uniform,
  title={On the uniform convergence of relative frequencies of events to their probabilities},
  author={Vapnik, Vladimir N and Chervonenkis, Alexey Y},
  journal={Theory of Probability and its Applications},
  volume={16},
  number={2},
  pages={264--280},
  year={1971}
}

@article{valiant1984theory,
  title={A theory of the learnable},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={27},
  number={11},
  pages={1134--1142},
  year={1984}
}

@article{blumer1989learnability,
  title={Learnability and the Vapnik-Chervonenkis dimension},
  author={Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth, Manfred K},
  journal={Journal of the ACM},
  volume={36},
  number={4},
  pages={929--965},
  year={1989}
}

@book{vapnik1998statistical,
  title={Statistical Learning Theory},
  author={Vapnik, Vladimir N},
  year={1998},
  publisher={Wiley}
}

% Комбинаторный подход

@article{vorontsov2004combinatorial,
  title={Комбинаторный подход к оценке качества обучаемых алгоритмов},
  author={Воронцов, Константин В},
  journal={Математические вопросы кибернетики},
  volume={13},
  pages={5--51},
  year={2004}
}

% Сложность Радемахера и метрические энтропии

@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  pages={463--482},
  year={2002}
}

@article{kolmogorov1961epsilon,
  title={\textit{$\varepsilon$}-entropy and \textit{$\varepsilon$}-capacity of sets in functional spaces},
  author={Kolmogorov, Andrey N and Tikhomirov, Vladimir M},
  journal={Uspekhi Matematicheskikh Nauk},
  volume={16},
  number={2},
  pages={3--86},
  year={1961}
}

% Теория сложности нейронных сетей

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of Control, Signals and Systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989}
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural Networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989}
}

@article{cybenko1990complexity,
  title={Complexity theory of neural networks and classification problems},
  author={Cybenko, George},
  journal={Neural Networks},
  volume={3},
  number={3},
  pages={251--258},
  year={1990}
}

@article{baum1989size,
  title={What size net gives valid generalization?},
  author={Baum, Eric B and Haussler, David},
  journal={Neural Computation},
  volume={1},
  number={1},
  pages={151--160},
  year={1989}
}

% Ландшафтные меры и геометрические подходы

@article{li2018visualizing,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@article{kiselev2024unraveling,
  title={Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes},
  author={Nikita Kiselev and Andrey Grabovoy},
  journal={Doklady Mathematics},
  volume={110},
  number={S1},
  pages={S42},
  year={2025},
  publisher={Springer}
}

@article{chaudhari2019entropy,
  title={Entropy-SGD: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997}
}

% Устойчивость (Stability)

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002}
}

@inproceedings{hardt2016train,
    author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
    title = {Train faster, generalize better: stability of stochastic gradient descent},
    year = {2016},
    publisher = {JMLR.org},
    booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
    pages = {1225–1234},
    numpages = {10},
    location = {New York, NY, USA},
    series = {ICML'16}
}

@InProceedings{charles2017stability,
  title = 	 {Stability and Generalization of Learning Algorithms that Converge to Global Optima},
  author =       {Charles, Zachary and Papailiopoulos, Dimitris},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {745--754},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/charles18a/charles18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/charles18a.html},
}


% Информационно-теоретические подходы

@article{russo2016controlling,
  title={Controlling bias in adaptive data analysis using information theory},
  author={Russo, Daniel and Zou, James},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@article{xu2017information,
  title={Information-theoretic analysis of generalization capability of learning algorithms},
  author={Xu, Aolin and Raginsky, Maxim},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{tishby1999information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={1999}
}

@article{shamir2010learnability,
  title={Learnability and stability in the general learning setting},
  author={Shamir, Ohad and Sabato, Sivan and Tishby, Naftali},
  journal={Conference on Learning Theory},
  pages={463--482},
  year={2010}
}

% Концентрационные неравенства

@book{boucheron2013concentration,
  title={Concentration Inequalities: A Nonasymptotic Theory of Independence},
  author={Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
  year={2013},
  publisher={Oxford University Press}
}

@book{wainwright2019high,
  title={High-Dimensional Statistics: A Non-Asymptotic Viewpoint},
  author={Wainwright, Martin J},
  year={2019},
  publisher={Cambridge University Press}
}

% Стохастическая оптимизация и обобщение

@inproceedings{bottou2008tradeoffs,
 author = {Bottou, L\'{e}on and Bousquet, Olivier},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Tradeoffs of Large Scale Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf},
 volume = {20},
 year = {2007}
}

@misc{lei2020stability,
      title={Stability and Generalization of Stochastic Gradient Methods for Minimax Problems}, 
      author={Yunwen Lei and Zhenhuan Yang and Tianbao Yang and Yiming Ying},
      year={2021},
      eprint={2105.03793},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2105.03793}, 
}

% Байесовские и PAC-Байесовские методы

@article{mcallester1999pac,
  title={Some PAC-Bayesian theorems},
  author={McAllester, David A},
  journal={Machine Learning},
  volume={37},
  number={3},
  pages={355--363},
  year={1999}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={Uncertainty in Artificial Intelligence},
  year={2017}
}

@article{hoffman2013stochastic,
  title={Stochastic variational inference},
  author={Hoffman, Matthew D and Blei, David M and Wang, Chong and Paisley, John},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={1303--1347},
  year={2013}
}

% Законы масштабирования

@article{hestness2017deep,
  title={Deep Learning Scaling is Predictable, Empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{hoffman2022training,
    author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack W. and Sifre, Laurent},
    title = {Training compute-optimal large language models},
    year = {2022},
    isbn = {9781713871088},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
    articleno = {2176},
    numpages = {15},
    location = {New Orleans, LA, USA},
    series = {NIPS '22}
}

% Дополнительные классические работы

@article{anthony1999neural,
  title={Neural Network Learning: Theoretical Foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={1999},
  publisher={Cambridge University Press}
}

@article{kearns1994introduction,
  title={An Introduction to Computational Learning Theory},
  author={Kearns, Michael J and Vazirani, Umesh V},
  year={1994},
  publisher={MIT Press}
}

@article{shalev2014understanding,
  title={Understanding Machine Learning: From Theory to Algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge University Press}
}

% Дополнительные работы по VC-теории

@article{dudley1978central,
  title={Central limit theorems for empirical measures},
  author={Dudley, Richard M},
  journal={Annals of Probability},
  volume={6},
  number={6},
  pages={899--929},
  year={1978}
}

@article{pollard1984convergence,
  title={Convergence of Stochastic Processes},
  author={Pollard, David},
  year={1984},
  publisher={Springer}
}

% Дополнительные работы по Радемахеру

@article{koltchinskii2001rademacher,
  title={Rademacher penalties and structural risk minimization},
  author={Koltchinskii, Vladimir},
  journal={IEEE Transactions on Information Theory},
  volume={47},
  number={5},
  pages={1902--1914},
  year={2001}
}

@article{ledoux2001concentration,
  title={The Concentration of Measure Phenomenon},
  author={Ledoux, Michel},
  year={2001},
  publisher={American Mathematical Society}
}

% Дополнительные работы по нейронным сетям

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information Theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993}
}

@article{maass1997networks,
  title={Networks of spiking neurons: The third generation of neural network models},
  author={Maass, Wolfgang},
  journal={Neural Networks},
  volume={10},
  number={9},
  pages={1659--1671},
  year={1997}
}

@article{anthony1993computational,
  title={Computational Learning Theory: An Introduction},
  author={Anthony, Martin and Biggs, Norman},
  year={1993},
  publisher={Cambridge University Press}
}

% Дополнительные работы по устойчивости

@article{shalev2010learnability,
    author = {Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
    title = {Learnability, Stability and Uniform Convergence},
    year = {2010},
    issue_date = {3/1/2010},
    publisher = {JMLR.org},
    volume = {11},
    issn = {1532-4435},
    month = dec,
    pages = {2635–2670},
    numpages = {36}
}

@inproceedings{kutin2002almost,
    author = {Kutin, Samuel and Niyogi, Partha},
    title = {Almost-everywhere algorithmic stability and generalization error},
    year = {2002},
    isbn = {1558608974},
    publisher = {Morgan Kaufmann Publishers Inc.},
    address = {San Francisco, CA, USA},
    booktitle = {Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence},
    pages = {275–282},
    numpages = {8},
    location = {Alberta, Canada},
    series = {UAI'02}
}

% Дополнительные работы по информационной теории

@article{negrea2019information,
  title={Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates},
  author={Negrea, Jeffrey and Haghifam, Mahdi and Dziugaite, Gintare Karolina and Khisti, Ashish and Roy, Daniel M},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@InProceedings{li2020understanding,
  title = 	 {Understanding Generalization in Deep Learning via Tensor Methods},
  author =       {Li, Jingling and Sun, Yanchao and Su, Jiahao and Suzuki, Taiji and Huang, Furong},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {504--515},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/li20c/li20c.pdf},
  url = 	 {https://proceedings.mlr.press/v108/li20c.html},
}

% Дополнительные работы по PAC-Байесу

@article{seeger2002pac,
  title={PAC-Bayesian generalisation error bounds for Gaussian process classification},
  author={Seeger, Matthias},
  journal={Journal of Machine Learning Research},
  volume={3},
  pages={233--269},
  year={2002}
}

@article{catoni2007pac,
  title={PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning},
  author={Catoni, Olivier},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}

% Дополнительные работы по оптимизации

@article{zhang2017understanding,
    author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
    title = {Understanding deep learning (still) requires rethinking generalization},
    year = {2021},
    issue_date = {March 2021},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {64},
    number = {3},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3446776},
    doi = {10.1145/3446776},
    journal = {Commun. ACM},
    month = feb,
    pages = {107–115},
    numpages = {9}
}

@InProceedings{arora2018stronger,
  title = 	 {Stronger Generalization Bounds for Deep Nets via a Compression Approach},
  author =       {Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {254--263},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/arora18b/arora18b.pdf},
  url = 	 {https://proceedings.mlr.press/v80/arora18b.html},
}


% Дополнительные работы по масштабированию

@article{henighan2020scaling,
  author       = {Tom Henighan and
                  Jared Kaplan and
                  Mor Katz and
                  Mark Chen and
                  Christopher Hesse and
                  Jacob Jackson and
                  Heewoo Jun and
                  Tom B. Brown and
                  Prafulla Dhariwal and
                  Scott Gray and
                  Chris Hallacy and
                  Benjamin Mann and
                  Alec Radford and
                  Aditya Ramesh and
                  Nick Ryder and
                  Daniel M. Ziegler and
                  John Schulman and
                  Dario Amodei and
                  Sam McCandlish},
  title        = {Scaling Laws for Autoregressive Generative Modeling},
  journal      = {CoRR},
  volume       = {abs/2010.14701},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.14701},
  eprinttype    = {arXiv},
  eprint       = {2010.14701},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-14701.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% Работы по трансформерам

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{dong2021attention,
  title={Attention is not all you need: Pure attention loses rank doubly exponentially with depth},
  author={Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  journal={International Conference on Machine Learning},
  pages={2793--2803},
  year={2021}
}

% Работы по диффузионным моделям

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{song2021score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={International Conference on Learning Representations},
  year={2021}
}

% Работы по метрическим энтропиям

@article{dudley1999uniform,
  title={Uniform Central Limit Theorems},
  author={Dudley, Richard M},
  year={1999},
  publisher={Cambridge University Press}
}

@article{van1996weak,
  title={Weak Convergence and Empirical Processes: With Applications to Statistics},
  author={van der Vaart, Aad W and Wellner, Jon A},
  year={1996},
  publisher={Springer}
}

% Работы по гессиану и кривизне

@misc{sagun2017eigenvalues,
      title={Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond}, 
      author={Levent Sagun and Leon Bottou and Yann LeCun},
      year={2017},
      eprint={1611.07476},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.07476}, 
}

@article{pennington2017resurrecting,
  title={Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice},
  author={Pennington, Jeffrey and Schoenholz, Samuel S and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2017}
}

% Работы по дифференциальной приватности

@InProceedings{dwork2006calibrating,
    author="Dwork, Cynthia
    and McSherry, Frank
    and Nissim, Kobbi
    and Smith, Adam",
    editor="Halevi, Shai
    and Rabin, Tal",
    title="Calibrating Noise to Sensitivity in Private Data Analysis",
    booktitle="Theory of Cryptography",
    year="2006",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="265--284",
    isbn="978-3-540-32732-5"
}

@inproceedings{bassily2016algorithmic,
    author = {Bassily, Raef and Nissim, Kobbi and Smith, Adam and Steinke, Thomas and Stemmer, Uri and Ullman, Jonathan},
    title = {Algorithmic stability for adaptive data analysis},
    year = {2016},
    isbn = {9781450341325},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2897518.2897566},
    doi = {10.1145/2897518.2897566},
    booktitle = {Proceedings of the Forty-Eighth Annual ACM Symposium on Theory of Computing},
    pages = {1046–1059},
    numpages = {14},
    keywords = {Adaptivity, Data Analysis, Differential Privacy, Generalization, Stability, Statistics},
    location = {Cambridge, MA, USA},
    series = {STOC '16}
}

% Работы по sample complexity

@article{hanneke2016optimal,
    author = {Hanneke, Steve},
    title = {The optimal sample complexity OF PAC learning},
    year = {2016},
    issue_date = {January 2016},
    publisher = {JMLR.org},
    volume = {17},
    number = {1},
    issn = {1532-4435},
    abstract = {This work establishes a new upper bound on the number of samples sufficient for PAC learning in the realizable case. The bound matches known lower bounds up to numerical constant factors. This solves a long-standing open problem on the sample complexity of PAC learning. The technique and analysis build on a recent breakthrough by Hans Simon.},
    journal = {J. Mach. Learn. Res.},
    month = jan,
    pages = {1319–1333},
    numpages = {15},
    keywords = {PAC learning, learning algorithm, minimax analysis, sample complexity, statistical learning theory}
}

% Работы по металёрнингу

@article{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  journal={International Conference on Machine Learning},
  pages={1126--1135},
  year={2017}
}

@article{raghu2020rapid,
  title={Rapid learning or feature reuse? Towards understanding the effectiveness of MAML},
  author={Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
  journal={International Conference on Learning Representations},
  year={2020}
}

% Работы по трансферному обучению

@article{pan2009survey,
  title={A survey on transfer learning},
  author={Pan, Sinno Jialin and Yang, Qiang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={22},
  number={10},
  pages={1345--1359},
  year={2009}
}

@article{ben2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine Learning},
  volume={79},
  number={1},
  pages={151--175},
  year={2010}
}

% Работы по состязательным атакам

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}

% Современные работы по обобщению

@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{jiang2020fantastic,
  title={Fantastic generalization measures and where to find them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  journal={International Conference on Learning Representations},
  year={2020}
}

% Работы по double descent

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019}
}

@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={12},
  pages={124003},
  year={2021}
}

% Работы по implicit bias

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018}
}

@misc{gunasekar2018implicit,
      title={Implicit Regularization in Matrix Factorization}, 
      author={Suriya Gunasekar and Blake Woodworth and Srinadh Bhojanapalli and Behnam Neyshabur and Nathan Srebro},
      year={2017},
      eprint={1705.09280},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1705.09280}, 
}

% Дополнительные современные работы по обобщению

@article{neyshabur2015norm,
  title={Norm-based capacity control in neural networks},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={Conference on Learning Theory},
  pages={1376--1401},
  year={2015}
}

@article{golowich2018size,
  title={Size-independent sample complexity of neural networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  journal={Conference on Learning Theory},
  pages={297--299},
  year={2018}
}

@article{wei2019data,
  title={Data-dependent sample complexity of deep neural networks via Lipschitz augmentation},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{neyshabur2017geometry,
  title={Geometry of Optimization and Implicit Regularization in Deep Learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Salakhutdinov, Ruslan and Srebro, Nathan},
  journal={arXiv Preprint},
  year={2017},
  url_Paper={https://arxiv.org/pdf/1705.03071.pdf}
}

@article{liang2019just,
  title={Just train longer: Generalizing better with longer training},
  author={Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Mukherjee, Rahul},
  journal={arXiv preprint arXiv:1905.13214},
  year={2019}
}

@article{neyshabur2020what,
  title={What is being transferred in transfer learning?},
  author={Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={512--523},
  year={2020}
}

@article{li2020neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Li, Zhiyuan and Wang, Ruosong and Yu, Dingli and Du, Simon S and Hu, Wei and Salakhutdinov, Russ R and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8592--8603},
  year={2020}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{allen2019convergence,
  title={Convergence of the dynamics of a class of gradient descent-ascent algorithms},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={International Conference on Machine Learning},
  pages={242--251},
  year={2019}
}

@article{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019}
}

@article{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  journal={International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}

@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, L{\'e}na{\"i}c and Oyallon, Edouard and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2019}
}

@InProceedings{woodworth2020kernel,
  title = 	 {Kernel and Rich Regimes in Overparametrized Models},
  author =       {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {3635--3673},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/woodworth20a/woodworth20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/woodworth20a.html},
}


% Работы по прореживанию и квантизации

@inproceedings{lecun1989optimal,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Damage},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
 volume = {2},
 year = {1989}
}

@inproceedings{han2015learning,
 author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning both Weights and Connections for Efficient Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf},
 volume = {28},
 year = {2015}
}

@misc{li2016pruning,
      title={Pruning Filters for Efficient ConvNets}, 
      author={Hao Li and Asim Kadav and Igor Durdanovic and Hanan Samet and Hans Peter Graf},
      year={2017},
      eprint={1608.08710},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1608.08710}, 
}

@article{he2017channel,
  title={Channel pruning for accelerating very deep neural networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  journal={International Conference on Computer Vision},
  pages={1389--1397},
  year={2017}
}

@inproceedings{
    wang2019picking,
    title={Picking Winning Tickets Before Training by Preserving Gradient Flow},
    author={Chaoqi Wang and Guodong Zhang and Roger Grosse},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=SkgsACVKPH}
}

@inproceedings{rastegari2016xnor,
    Author = {Mohammad Rastegari and Vicente Ordonez and Joseph Redmon and Ali Farhadi},
    Title = {XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks},
    Booktitle = {ECCV},
    Year = {2016}
}

@article{hubara2017quantized,
    author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
    title = {Quantized neural networks: training neural networks with low precision weights and activations},
    year = {2017},
    issue_date = {January 2017},
    publisher = {JMLR.org},
    volume = {18},
    number = {1},
    issn = {1532-4435},
    journal = {J. Mach. Learn. Res.},
    month = jan,
    pages = {6869–6898},
    numpages = {30},
    keywords = {computer vision, deep learning, energy efficient neural networks, language models, neural networks compression}
}

@article{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  journal={Conference on Computer Vision and Pattern Recognition},
  pages={2704--2713},
  year={2018}
}

@inproceedings{wang2019haq,
    author = {Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
    title = {HAQ: Hardware-Aware Automated Quantization With Mixed Precision},
    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2019}
}

@article{zhou2016dorefa,
  author    = {Shuchang Zhou and Yuxin Wu and Zekun Ni and Xinyu Zhou and He Wen and Yuheng Zou},
  title     = {DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients},
  journal   = {CoRR},
  volume    = {abs/1606.06160},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.06160},
}

