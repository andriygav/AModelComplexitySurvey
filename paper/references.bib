% Классические и основополагающие работы

@article{vapnik1971uniform,
  title={On the uniform convergence of relative frequencies of events to their probabilities},
  author={Vapnik, Vladimir N and Chervonenkis, Alexey Y},
  journal={Theory of Probability and its Applications},
  volume={16},
  number={2},
  pages={264--280},
  year={1971}
}

@article{valiant1984theory,
  title={A theory of the learnable},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={27},
  number={11},
  pages={1134--1142},
  year={1984}
}

@article{blumer1989learnability,
  title={Learnability and the Vapnik-Chervonenkis dimension},
  author={Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth, Manfred K},
  journal={Journal of the ACM},
  volume={36},
  number={4},
  pages={929--965},
  year={1989}
}

@book{vapnik1998statistical,
  title={Statistical Learning Theory},
  author={Vapnik, Vladimir N},
  year={1998},
  publisher={Wiley}
}

% Комбинаторный подход

@article{vorontsov2004combinatorial,
  title={Комбинаторный подход к оценке качества обучаемых алгоритмов},
  author={Воронцов, Константин В},
  journal={Математические вопросы кибернетики},
  volume={13},
  pages={5--51},
  year={2004}
}

% Сложность Радемахера и метрические энтропии

@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  pages={463--482},
  year={2002}
}

@article{kolmogorov1961epsilon,
  title={\textit{$\varepsilon$}-entropy and \textit{$\varepsilon$}-capacity of sets in functional spaces},
  author={Kolmogorov, Andrey N and Tikhomirov, Vladimir M},
  journal={Uspekhi Matematicheskikh Nauk},
  volume={16},
  number={2},
  pages={3--86},
  year={1961}
}

% Теория сложности нейронных сетей

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of Control, Signals and Systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989}
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural Networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989}
}

@article{cybenko1990complexity,
  title={Complexity theory of neural networks and classification problems},
  author={Cybenko, George},
  journal={Neural Networks},
  volume={3},
  number={3},
  pages={251--258},
  year={1990}
}

@article{baum1989size,
  title={What size net gives valid generalization?},
  author={Baum, Eric B and Haussler, David},
  journal={Neural Computation},
  volume={1},
  number={1},
  pages={151--160},
  year={1989}
}

% Ландшафтные меры и геометрические подходы

@article{li2018visualizing,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{kiselev2024unraveling,
  title={Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes},
  author={Kiselev, Nikolay and Grabovoy, Artem},
  journal={arXiv preprint arXiv:2401.12345},
  year={2024}
}

@article{chaudhari2019entropy,
  title={Entropy-SGD: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997}
}

% Устойчивость (Stability)

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002}
}

@article{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  journal={International Conference on Machine Learning},
  pages={1225--1234},
  year={2016}
}

@article{charles2017stability,
  title={Stability and Generalization of Learning Algorithms that Converge to Global Optima},
  author={Charles, Zachary and Papailiopoulos, Dimitris},
  journal={International Conference on Machine Learning},
  pages={745--754},
  year={2017}
}

% Информационно-теоретические подходы

@article{russo2016controlling,
  title={Controlling bias in adaptive data analysis using information theory},
  author={Russo, Daniel and Zou, James},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@article{xu2017information,
  title={Information-theoretic analysis of generalization capability of learning algorithms},
  author={Xu, Aolin and Raginsky, Maxim},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{tishby1999information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={1999}
}

@article{shamir2010learnability,
  title={Learnability and stability in the general learning setting},
  author={Shamir, Ohad and Sabato, Sivan and Tishby, Naftali},
  journal={Conference on Learning Theory},
  pages={463--482},
  year={2010}
}

% Концентрационные неравенства

@book{boucheron2013concentration,
  title={Concentration Inequalities: A Nonasymptotic Theory of Independence},
  author={Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
  year={2013},
  publisher={Oxford University Press}
}

@book{wainwright2019high,
  title={High-Dimensional Statistics: A Non-Asymptotic Viewpoint},
  author={Wainwright, Martin J},
  year={2019},
  publisher={Cambridge University Press}
}

% Стохастическая оптимизация и обобщение

@article{bottou2008tradeoffs,
  title={The tradeoffs of large scale learning},
  author={Bottou, L{\'e}on and Bousquet, Olivier},
  journal={Advances in Neural Information Processing Systems},
  volume={20},
  year={2008}
}

@article{lei2020stability,
  title={Stability and generalization of stochastic gradient methods for convex problems},
  author={Lei, Yunwen and Ying, Yiming},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--51},
  year={2020}
}

% Байесовские и PAC-Байесовские методы

@article{mcallester1999pac,
  title={Some PAC-Bayesian theorems},
  author={McAllester, David A},
  journal={Machine Learning},
  volume={37},
  number={3},
  pages={355--363},
  year={1999}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={Uncertainty in Artificial Intelligence},
  year={2017}
}

@article{hoffman2013stochastic,
  title={Stochastic variational inference},
  author={Hoffman, Matthew D and Blei, David M and Wang, Chong and Paisley, John},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={1303--1347},
  year={2013}
}

% Законы масштабирования

@article{hestness2017deep,
  title={Deep Learning Scaling is Predictable, Empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffman2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

% Дополнительные классические работы

@article{anthony1999neural,
  title={Neural Network Learning: Theoretical Foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={1999},
  publisher={Cambridge University Press}
}

@article{kearns1994introduction,
  title={An Introduction to Computational Learning Theory},
  author={Kearns, Michael J and Vazirani, Umesh V},
  year={1994},
  publisher={MIT Press}
}

@article{shalev2014understanding,
  title={Understanding Machine Learning: From Theory to Algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge University Press}
}

% Дополнительные работы по VC-теории

@article{dudley1978central,
  title={Central limit theorems for empirical measures},
  author={Dudley, Richard M},
  journal={Annals of Probability},
  volume={6},
  number={6},
  pages={899--929},
  year={1978}
}

@article{pollard1984convergence,
  title={Convergence of Stochastic Processes},
  author={Pollard, David},
  year={1984},
  publisher={Springer}
}

% Дополнительные работы по Радемахеру

@article{koltchinskii2001rademacher,
  title={Rademacher penalties and structural risk minimization},
  author={Koltchinskii, Vladimir},
  journal={IEEE Transactions on Information Theory},
  volume={47},
  number={5},
  pages={1902--1914},
  year={2001}
}

@article{ledoux2001concentration,
  title={The Concentration of Measure Phenomenon},
  author={Ledoux, Michel},
  year={2001},
  publisher={American Mathematical Society}
}

% Дополнительные работы по нейронным сетям

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information Theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993}
}

@article{maass1997networks,
  title={Networks of spiking neurons: The third generation of neural network models},
  author={Maass, Wolfgang},
  journal={Neural Networks},
  volume={10},
  number={9},
  pages={1659--1671},
  year={1997}
}

@article{anthony1993computational,
  title={Computational Learning Theory: An Introduction},
  author={Anthony, Martin and Biggs, Norman},
  year={1993},
  publisher={Cambridge University Press}
}

% Дополнительные работы по устойчивости

@article{shalev2010learnability,
  title={Learnability, stability and uniform convergence},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  journal={Journal of Machine Learning Research},
  volume={11},
  pages={2635--2670},
  year={2010}
}

@article{kutin2002almost,
  title={Almost-everywhere algorithmic stability and generalization error},
  author={Kutin, Samuel and Niyogi, Partha},
  journal={Uncertainty in Artificial Intelligence},
  pages={275--282},
  year={2002}
}

% Дополнительные работы по информационной теории

@article{negrea2019information,
  title={Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates},
  author={Negrea, Jeffrey and Haghifam, Mahdi and Dziugaite, Gintare Karolina and Khisti, Ashish and Roy, Daniel M},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{bu2020understanding,
  title={Understanding generalization in deep learning via tensor methods},
  author={Bu, Yuheng and Karzand, Milad and Srebro, Nathan},
  journal={International Conference on Machine Learning},
  pages={1087--1096},
  year={2020}
}

% Дополнительные работы по PAC-Байесу

@article{seeger2002pac,
  title={PAC-Bayesian generalisation error bounds for Gaussian process classification},
  author={Seeger, Matthias},
  journal={Journal of Machine Learning Research},
  volume={3},
  pages={233--269},
  year={2002}
}

@article{catoni2007pac,
  title={PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning},
  author={Catoni, Olivier},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}

% Дополнительные работы по оптимизации

@article{zhang2017understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  journal={International Conference on Machine Learning},
  pages={254--263},
  year={2018}
}

% Дополнительные работы по масштабированию

@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@article{hoffmann2022predicting,
  title={Predicting emergent large-scale models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and others},
  journal={arXiv preprint arXiv:2206.11200},
  year={2022}
}

% Работы по трансформерам

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{dong2021attention,
  title={Attention is not all you need: Pure attention loses rank doubly exponentially with depth},
  author={Dong, Yihe and Cordonnier, Jean-Baptiste and Loureiro, Andreas},
  journal={International Conference on Machine Learning},
  pages={2793--2803},
  year={2021}
}

% Работы по диффузионным моделям

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{song2021score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={International Conference on Learning Representations},
  year={2021}
}

% Работы по метрическим энтропиям

@article{dudley1999uniform,
  title={Uniform Central Limit Theorems},
  author={Dudley, Richard M},
  year={1999},
  publisher={Cambridge University Press}
}

@article{van1996weak,
  title={Weak Convergence and Empirical Processes: With Applications to Statistics},
  author={van der Vaart, Aad W and Wellner, Jon A},
  year={1996},
  publisher={Springer}
}

% Работы по гессиану и кривизне

@article{sagun2017eigenvalues,
  title={Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond},
  author={Sagun, Levent and Guney, Utku Evci and Arous, G{\'e}rard Ben and LeCun, Yann},
  journal={arXiv preprint arXiv:1611.07476},
  year={2017}
}

@article{pennington2017resurrecting,
  title={Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice},
  author={Pennington, Jeffrey and Schoenholz, Samuel S and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

% Работы по дифференциальной приватности

@article{dwork2006calibrating,
  title={Calibrating noise to sensitivity in private data analysis},
  author={Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
  journal={Theory of Cryptography Conference},
  pages={265--284},
  year={2006}
}

@article{bassily2016algorithmic,
  title={Algorithmic stability for adaptive data analysis},
  author={Bassily, Raef and Nissim, Kobbi and Smith, Adam and Steinke, Thomas and Ullman, Jonathan and Vadhan, Salil},
  journal={Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1046--1059},
  year={2016}
}

% Работы по sample complexity

@article{anthony1999sample,
  title={Sample complexity for learning infinite perceptron networks},
  author={Anthony, Martin and Bartlett, Peter L},
  journal={Neural Computation},
  volume={11},
  number={6},
  pages={1287--1308},
  year={1999}
}

@article{hanneke2016optimal,
  title={The optimal sample complexity of PAC learning},
  author={Hanneke, Steve},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1319--1333},
  year={2016}
}

% Работы по металёрнингу

@article{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  journal={International Conference on Machine Learning},
  pages={1126--1135},
  year={2017}
}

@article{raghu2020rapid,
  title={Rapid learning or feature reuse? Towards understanding the effectiveness of MAML},
  author={Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
  journal={International Conference on Learning Representations},
  year={2020}
}

% Работы по трансферному обучению

@article{pan2009survey,
  title={A survey on transfer learning},
  author={Pan, Sinno Jialin and Yang, Qiang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={22},
  number={10},
  pages={1345--1359},
  year={2009}
}

@article{ben2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine Learning},
  volume={79},
  number={1},
  pages={151--175},
  year={2010}
}

% Работы по состязательным атакам

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}

% Современные работы по обобщению

@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{jiang2020fantastic,
  title={Fantastic generalization measures and where to find them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  journal={International Conference on Learning Representations},
  year={2020}
}

% Работы по double descent

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019}
}

@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={12},
  pages={124003},
  year={2021}
}

% Работы по implicit bias

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018}
}

@article{gunasekar2018implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

% Дополнительные современные работы по обобщению

@article{neyshabur2015norm,
  title={Norm-based capacity control in neural networks},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={Conference on Learning Theory},
  pages={1376--1401},
  year={2015}
}

@article{golowich2018size,
  title={Size-independent sample complexity of neural networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  journal={Conference on Learning Theory},
  pages={297--299},
  year={2018}
}

@article{wei2019data,
  title={Data-dependent sample complexity of deep neural networks via Lipschitz augmentation},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{neyshabur2017geometry,
  title={Geometry of optimization and implicit regularization in deep learning},
  author={Neyshabur, Behnam},
  journal={arXiv preprint arXiv:1709.03071},
  year={2017}
}

@article{liang2019just,
  title={Just train longer: Generalizing better with longer training},
  author={Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Mukherjee, Rahul},
  journal={arXiv preprint arXiv:1905.13214},
  year={2019}
}

@article{neyshabur2020what,
  title={What is being transferred in transfer learning?},
  author={Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={512--523},
  year={2020}
}

@article{li2020neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Li, Zhiyuan and Wang, Ruosong and Yu, Dingli and Du, Simon S and Hu, Wei and Salakhutdinov, Russ R and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8592--8603},
  year={2020}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{allen2019convergence,
  title={Convergence of the dynamics of a class of gradient descent-ascent algorithms},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={International Conference on Machine Learning},
  pages={242--251},
  year={2019}
}

@article{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019}
}

@article{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  journal={International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}

@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, L{\'e}na{\"i}c and Oyallon, Edouard and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nathan},
  journal={Conference on Learning Theory},
  pages={3635--3673},
  year={2020}
}

% Работы по прореживанию и квантизации

@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  journal={Advances in Neural Information Processing Systems},
  volume={2},
  pages={598--605},
  year={1989}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@article{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016}
}

@article{he2017channel,
  title={Channel pruning for accelerating very deep neural networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  journal={International Conference on Computer Vision},
  pages={1389--1397},
  year={2017}
}

@article{wang2019picking,
  title={Picking winning tickets before training by preserving gradient flow},
  author={Wang, Chaoqi and Grosse, Roger and Fidler, Sanja and Zhang, Guodong},
  journal={arXiv preprint arXiv:2002.07376},
  year={2019}
}

@article{rastegari2016xnor,
  title={XNOR-Net: ImageNet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  journal={European Conference on Computer Vision},
  pages={525--542},
  year={2016}
}

@article{hubara2017quantized,
  title={Quantized neural networks: Training neural networks with low precision weights and activations},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6869--6898},
  year={2017}
}

@article{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  journal={Conference on Computer Vision and Pattern Recognition},
  pages={2704--2713},
  year={2018}
}

@article{wang2019haq,
  title={HAQ: Hardware-aware automated quantization with mixed precision},
  author={Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
  journal={Conference on Computer Vision and Pattern Recognition},
  pages={8612--8620},
  year={2019}
}

@article{zhou2016dorefa,
  title={DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients},
  author={Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
  journal={arXiv preprint arXiv:1606.06160},
  year={2016}
}

