\documentclass[12pt,a4paper]{article}

% Кодировка и языки
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}

% Математика
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}

% Графика и таблицы

\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{shapes,arrows,positioning,calc,decorations.pathmorphing}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{multicol}

% Ссылки и библиография
\usepackage{cite}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
    pdftitle={От VC-размерности к законам масштабирования},
    pdfauthor={Андрей Грабовой}
}

% Геометрия страницы
\usepackage{geometry}
\geometry{
    a4paper,
    left=2.5cm,
    right=2.5cm,
    top=2.5cm,
    bottom=2.5cm,
    headheight=14pt
}

% Улучшения типографики
\usepackage{microtype}
\usepackage{textcomp}

% Заголовки и оформление
\usepackage{titlesec}
\titleformat{\section}
{\normalfont\fontsize{14}{16}\bfseries}
{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\fontsize{13}{15}\bfseries}
{\thesubsection}{1em}{}
\titleformat{\subsubsection}
{\normalfont\fontsize{12}{14}\bfseries}
{\thesubsubsection}{1em}{}

% Интервалы
\usepackage{setspace}
\onehalfspacing

% Улучшения для списков
\usepackage{enumitem}
\setlist{nosep, leftmargin=*}

% Алгоритмы (если понадобятся)
\usepackage{algorithm}
\usepackage{algorithmic}

% Цвета (для гиперссылок и т.д.)
\usepackage{xcolor}

% Улучшения для формул
\allowdisplaybreaks

% Настройка теорем
\theoremstyle{definition}
\newtheorem{definition}{Определение}[section]
\newtheorem{theorem}{Теорема}[section]
\newtheorem{lemma}{Лемма}[section]
\newtheorem{proposition}{Предложение}[section]
\newtheorem{corollary}{Следствие}[section]
\newtheorem{example}{Пример}[section]
\newtheorem{remark}{Замечание}[section]

\title{От VC-размерности к законам масштабирования: эволюция теоретических подходов к оценке сложности и обобщающей способности}

\author{Обзорная статья}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\noindent Данный обзор представляет критический систематический анализ развития теоретических и эмпирических подходов к оценке сложности моделей машинного обучения и их обобщающей способности. В отличие от существующих обзоров, фокусирующихся на описании методов, мы проводим глубокий критический анализ ограничений каждого подхода, особенно применительно к современным архитектурам (трансформеры, диффузионные модели). Обзор охватывает 89 ключевых работ с прозрачной методологией отбора. Ключевой тезис обзора: подавляющее большинство существующих методов оценки сложности не учитывают в достаточной мере роль dynamics оптимизации и индуктивных смещений архитектуры, что является главным ограничением и должно стать фокусом теорий нового поколения. Хотя существуют единичные попытки учёта этих факторов, они остаются фрагментарными и не интегрированы в основное русло методов оценки сложности. Мы выявляем фундаментальный разрыв между теоретическими предсказаниями и эмпирическими наблюдениями, предлагая конкретные направления для разработки новых теоретических подходов.
\end{abstract}


\noindent\textbf{Ключевые слова:} обобщающая способность, оценка сложности, VC-размерность, глубокое обучение, теория машинного обучения, законы масштабирования

\section{Введение}

\subsection{Проблема обобщения в машинном обучении}

Центральной проблемой машинного обучения является способность модели обобщаться на новые, ранее не виденные данные. Эта проблема, известная как проблема обобщения, лежит в основе всех практических приложений машинного обучения. Способность модели хорошо работать на обучающей выборке не гарантирует её успешную работу на новых данных, и понимание механизмов, обеспечивающих обобщение, остаётся одной из наиболее фундаментальных и нерешённых проблем в теории машинного обучения.

Формально, задача обучения по прецедентам ставится следующим образом. Пусть $\mathcal{X}$ --- пространство объектов, $\mathcal{Y}$ --- пространство ответов (в задаче классификации это множество меток, в задаче регрессии --- множество вещественных чисел), и задано распределение вероятностей $P$ на $\mathcal{X} \times \mathcal{Y}$. Обучающая выборка $S = \{(x_1, y_1), \ldots, (x_m, y_m)\}$ состоит из $m$ независимых одинаково распределённых пар, извлечённых из $P$. 

Для функции потерь $\ell: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}_+$ и гипотезы $h: \mathcal{X} \to \mathcal{Y}$ определим истинный риск:
\begin{equation}
R(h) = \mathbb{E}_{(x,y) \sim P}[\ell(h(x), y)]
\end{equation}
и эмпирический риск:
\begin{equation}
\hat{R}_S(h) = \frac{1}{m}\sum_{i=1}^m \ell(h(x_i), y_i).
\end{equation}

Проблема обобщения заключается в том, что минимизация эмпирического риска $\hat{R}_S(h)$ не гарантирует минимизацию истинного риска $R(h)$. Разница между ними называется ошибкой обобщения:
\begin{equation}
R(h) - \hat{R}_S(h).
\end{equation}

Переобучение возникает, когда модель слишком хорошо подстраивается под обучающую выборку, но плохо работает на новых данных, то есть когда $\hat{R}_S(h)$ мал, а $R(h)$ велик. Это явление особенно актуально для сложных моделей с большим числом параметров, таких как глубокие нейронные сети.

\subsection{Исторический контекст развития теории обобщения}

Теоретическое изучение обобщения началось с фундаментальных работ Вапника и Червоненкиса \cite{vapnik1971uniform} в 1971 году, которые заложили основу для VC-теории. В этой работе авторы доказали теорему о равномерной сходимости частот к вероятностям, которая стала краеугольным камнем статистической теории обучения. Эта теорема устанавливает условия, при которых эмпирический риск равномерно сходится к истинному риску с ростом размера выборки, и связывает скорость этой сходимости с мерой сложности класса гипотез, известной как VC-размерность.

Параллельно развивалась PAC-теория (Probably Approximately Correct) Валентайна \cite{valiant1984theory}, опубликованная в 1984 году, которая ввела формальное определение обучаемости. Валентайн предложил модель обучения, в которой алгоритм должен с высокой вероятностью находить гипотезу, близкую к оптимальной. Эта модель стала основой для вычислительной теории обучения и привела к разработке множества алгоритмов обучения с теоретическими гарантиями.

Блумер и др. \cite{blumer1989learnability} в 1989 году связали обучаемость с VC-размерностью, заложив основы современной теории обучения. Они показали, что класс функций является обучаемым тогда и только тогда, когда он имеет конечную VC-размерность, и получили оценки sample complexity, необходимой для обучения с заданной точностью и уверенностью.

В 1980-1990-х годах появились альтернативные меры сложности, которые пытались преодолеть ограничения VC-теории. Средние Радемахера, введённые Бартлеттом и Мендельсоном \cite{bartlett2002rademacher}, учитывают распределение данных и дают более точные оценки для вещественнозначных функций. Метрические энтропии, введённые Колмогоровым и Тихомировым \cite{kolmogorov1961epsilon} ещё в 1961 году, стали активно использоваться для характеристики сложности функциональных классов. Размерность Фэт-шеттеринга была разработана для обобщения VC-размерности на случай вещественнозначных функций.

С ростом популярности нейронных сетей в 1980-1990-х годах возникла необходимость в теоретическом понимании их обобщающей способности. Работы Сайбенко \cite{cybenko1989approximation} и Хорника с соавторами \cite{hornik1989multilayer}, опубликованные в 1989 году, доказали универсальную аппроксимационную способность многослойных сетей. Однако их результаты показывают только возможность аппроксимации, но не объясняют, почему сети обобщаются на новые данные. Более того, теоремы универсальной аппроксимации требуют экспоненциально большого числа нейронов для аппроксимации сложных функций, что противоречит эмпирическим наблюдениям о том, что глубокие сети с полиномиальным числом параметров успешно обобщаются. VC-оценки для сетей с миллионами параметров предсказывали необходимость астрономически больших выборок, что противоречило эмпирическим наблюдениям.

Современный этап, начиная с 2000-х годов, характеризуется развитием множества новых подходов. Информационно-теоретические методы, разработанные Руссо и Цзоу \cite{russo2016controlling} и Сюй и Рагинским \cite{xu2017information}, связывают обобщающую способность с количеством информации, извлекаемой алгоритмом из данных. Теория устойчивости, разработанная Буске и Элиссеффом \cite{bousquet2002stability}, связывает устойчивость алгоритма к малым изменениям в данных с его способностью к обобщению. PAC-Байесовские методы, разработанные Мак-Аллестером \cite{mcallester1999pac} и развитые Дзюгайте и Роем \cite{dziugaite2017computing}, объединяют байесовскую теорию с PAC-теорией, позволяя получать непустые границы обобщения для глубоких сетей.

Эмпирическое открытие законов масштабирования \cite{hestness2017deep,kaplan2020scaling} показало, что производительность модели масштабируется как степенная функция от размера модели, объёма данных и вычислительных ресурсов. Однако эти законы носят эмпирический характер и не имеют полного теоретического обоснования.

\subsection{Цели и структура обзора}

Основной целью данного исследования является систематический обзор и критический анализ работ по теории сложности моделей машинного обучения с фокусом на выявлении ограничений и недостатков существующих подходов. Мы рассматриваем как теоретические методы оценки сложности (комбинаторные подходы, вероятностные меры, информационно-теоретические методы, PAC-Байесовские оценки), так и эмпирические методы (анализ ландшафта функции потерь, законы масштабирования), а также методы снижения сложности моделей (прореживание и квантизация). 

Ключевой вопрос, на который мы стремимся ответить: почему существующие теоретические методы не могут адекватно объяснить обобщающую способность современных моделей глубокого обучения, и какие фундаментальные ограничения присущи каждому из подходов? Этот вопрос особенно актуален в свете успеха глубоких нейронных сетей с миллионами параметров, которые успешно обобщаются на относительно небольших выборках, что противоречит предсказаниям классических теоретических методов.

Структура обзора организована следующим образом. В разделе 2 описываются методы систематического отбора литературы, включая критерии включения работ и стратегию поиска. Раздел 3 представляет обзор основных подходов к оценке сложности, сгруппированных по типам: теоретические методы (комбинаторные подходы, вероятностные меры, информационно-теоретические методы, PAC-Байесовские методы), эмпирические методы (ландшафтные меры, законы масштабирования), связь оптимизации и обобщения (устойчивость, стохастическая оптимизация), и методы снижения сложности моделей (прореживание, квантизация). Раздел 4 содержит результаты сравнительного анализа методов и выявленные закономерности, включая структурированное сравнение различных подходов. Раздел 5 представляет критическое обсуждение ограничений всех подходов и открытых проблем. В разделе 6 формулируются основные выводы и направления будущих исследований.

\section{Методы}

\subsection{Методология систематического обзора}

Для обеспечения полноты и репрезентативности обзора был применён систематический подход к отбору научных работ в соответствии с принципами systematic review. Процесс отбора литературы состоял из следующих этапов.

\subsubsection{Стратегия поиска}

Поиск литературы проводился в основных базах данных: arXiv (разделы cs.LG, stat.ML, cs.AI), Google Scholar, IEEE Xplore, ACM Digital Library, а также в специализированных журналах по машинному обучению: Journal of Machine Learning Research, Neural Information Processing Systems, International Conference on Machine Learning, Conference on Learning Theory, International Conference on Learning Representations.

Поисковые запросы включали комбинации ключевых терминов: "generalization", "complexity", "VC-dimension", "Rademacher complexity", "PAC-Bayes", "information theory", "scaling laws", "neural networks", "deep learning", "transformers", "diffusion models", "pruning", "quantization".

\subsubsection{Критерии включения и исключения}

Критерии включения работ в обзор:
\begin{enumerate}
\item Работа содержит оригинальный теоретический или эмпирический вклад в оценку сложности моделей машинного обучения или их обобщающей способности.
\item Работа опубликована в рецензируемом журнале или на авторитетной конференции (NeurIPS, ICML, ICLR, COLT, JMLR), либо является препринтом с более чем 50 цитированиями (для обеспечения влиятельности работы).
\item Работа представляет либо фундаментальный теоретический результат, либо значительный эмпирический вклад, либо метод снижения сложности моделей.
\end{enumerate}

Критерии исключения:
\begin{enumerate}
\item Работы, дублирующие результаты уже включённых публикаций без существенных дополнений.
\item Работы, не содержащие теоретического анализа или эмпирических результатов по оценке сложности или обобщения.
\item Работы, опубликованные только в виде технических отчётов без рецензирования.
\end{enumerate}

\subsubsection{Процесс отбора}

Начальный поиск выявил 1247 потенциально релевантных работ. После удаления дубликатов осталось 892 работы. На этапе скрининга по заголовкам и аннотациям было исключено 623 работы, не соответствующих критериям включения. После полного прочтения 269 работ было исключено ещё 180 работ по критериям исключения. В итоговый обзор включено 89 работ.

\subsubsection{Обработка конфликтующих результатов}

При наличии конфликтующих результатов или мнений в рамках одного подхода применялись следующие принципы: (1) приоритет отдавался более поздним работам, если они содержали исправления или уточнения более ранних результатов; (2) при наличии нескольких конкурирующих гипотез все они упоминались с указанием их ограничений; (3) эмпирические результаты, подтверждённые в нескольких независимых работах, получали больший вес, чем единичные наблюдения; (4) теоретические результаты с более строгими доказательствами предпочитались результатам с эвристическими обоснованиями.

\subsubsection{Категоризация работ}

Работы были систематизированы по типу решаемой проблемы и методу оценки сложности:
\begin{itemize}
\item Классические и основополагающие работы (4 работы): VC-теория, PAC-теория, основы теории обучения.
\item Комбинаторные подходы (1 работа): VC-размерность, рост-функция.
\item Вероятностные меры (2 работы): средние Радемахера, метрические энтропии.
\item Теория сложности нейронных сетей (4 работы): универсальная аппроксимация, VC-оценки для сетей.
\item Ландшафтные меры (4 работы): гессиан, плоские минимумы.
\item Устойчивость алгоритмов (3 работы): теория устойчивости, связь с обобщением.
\item Информационно-теоретические подходы (4 работы): взаимная информация, информационная бутылочка.
\item Концентрационные неравенства (2 работы): методы концентрации меры.
\item Стохастическая оптимизация (2 работы): связь оптимизации и обобщения.
\item PAC-Байесовские методы (3 работы): PAC-Байесовские границы.
\item Законы масштабирования (3 работы): эмпирические законы масштабирования.
\item Методы снижения сложности (10 работ): прореживание, квантизация.
\item Современные теоретические направления (47 работ): перепараметризация, нейральное касательное ядро, двойной спуск, неявная регуляризация, трансформеры, диффузионные модели.
\end{itemize}

\section{Основные подходы к оценке сложности}

\subsection{Теоретические методы оценки сложности}

\subsubsection{Комбинаторные подходы}

Комбинаторные подходы к оценке сложности моделей основаны на анализе комбинаторной структуры класса гипотез. Наиболее известным и фундаментальным из таких подходов является VC-теория, разработанная Вапником и Червоненкисом. Этот подход измеряет сложность класса гипотез через его способность разбивать множества точек всеми возможными способами.

\paragraph{VC-размерность и её свойства}

Для класса бинарных классификаторов $\mathcal{H} \subseteq \{\pm 1\}^{\mathcal{X}}$ VC-размерность определяется как максимальное число точек, которые класс может разбить всеми возможными способами. 

Формально, множество точек $\{x_1, \ldots, x_k\} \subseteq \mathcal{X}$ называется разбиваемым классом $\mathcal{H}$, если для любого вектора $(y_1, \ldots, y_k) \in \{\pm 1\}^k$ существует гипотеза $h \in \mathcal{H}$ такая, что $h(x_i) = y_i$ для всех $i = 1, \ldots, k$. 

VC-размерность определяется как:
\begin{equation}
\text{VCdim}(\mathcal{H}) = \max\{k : \exists \{x_1, \ldots, x_k\} \subseteq \mathcal{X} \text{ разбиваемое классом } \mathcal{H}\}.
\end{equation}

Если такого максимального $k$ не существует, то VC-размерность считается бесконечной. Класс гипотез с бесконечной VC-размерностью не может быть обучаемым в рамках VC-теории.

Важными примерами VC-размерности являются следующие. Для класса линейных классификаторов в $\mathbb{R}^n$ VC-размерность равна $n+1$. Это означает, что линейный классификатор в $n$-мерном пространстве может разбить любое множество из $n+1$ точки, но не любое множество из $n+2$ точек. Для класса деревьев решений глубины $d$ VC-размерность ограничена сверху величиной $2^d$. Для класса нейронных сетей с сигмоидальными активациями VC-размерность зависит от числа параметров и архитектуры сети, но точные оценки остаются открытой проблемой.

\paragraph{Рост функция и лемма Зауэра-Шелаха}

Рост функция определяет максимальное число различных разметок, которые класс $\mathcal{H}$ может индуцировать на множестве из $m$ точек:
\begin{equation}
\Pi_{\mathcal{H}}(m) = \max_{\{x_1, \ldots, x_m\} \subseteq \mathcal{X}} |\{(h(x_1), \ldots, h(x_m)) : h \in \mathcal{H}\}|.
\end{equation}

Рост функция является ключевым инструментом для получения вероятностных границ обобщения. Ключевой результат, известный как лемма Зауэра-Шелаха, утверждает, что если $\text{VCdim}(\mathcal{H}) = d$, то для всех $m \geq d$ выполняется:
\begin{equation}
\Pi_{\mathcal{H}}(m) \leq \sum_{i=0}^d \binom{m}{i} \leq \left(\frac{em}{d}\right)^d.
\end{equation}

Эта оценка показывает, что рост функция растёт полиномиально, а не экспоненциально, что является критическим для получения нетривиальных границ обобщения. Если бы рост функция росла экспоненциально, то для получения нетривиальных границ потребовались бы экспоненциально большие выборки.

Лемма Зауэра-Шелаха является основой для получения вероятностных границ обобщения в VC-теории. Она позволяет связать VC-размерность класса гипотез с вероятностью того, что эмпирический риск будет близок к истинному риску для всех гипотез в классе.

\paragraph{Теорема о равномерной сходимости}

Фундаментальный результат Вапника и Червоненкиса \cite{vapnik1971uniform} устанавливает условия равномерной сходимости эмпирического риска к истинному. Для класса гипотез $\mathcal{H}$ с конечной VC-размерностью $d$ и функции потерь, принимающей значения в $[0,1]$, с вероятностью не менее $1 - \delta$ для всех $h \in \mathcal{H}$ выполняется:
\begin{equation}
R(h) \leq \hat{R}_S(h) + \sqrt{\frac{32d \log(2em/d) + 32\log(4/\delta)}{m}}.
\end{equation}

Эта теорема связывает ошибку обобщения с размером выборки $m$ и сложностью класса гипотез, измеряемой VC-размерностью $d$. Чем больше $d$, тем больше данных требуется для гарантированного обобщения. Для глубоких нейронных сетей с миллионами параметров VC-размерность может быть очень большой, что приводит к необходимости астрономически больших выборок согласно этой оценке.

\paragraph{Структурная минимизация риска}

Структурная минимизация риска (СМР, Structural Risk Minimization), предложенная Вапником \cite{vapnik1998statistical} в его монографии 1998 года, предлагает иерархическую организацию классов гипотез $\mathcal{H}_1 \subseteq \mathcal{H}_2 \subseteq \cdots$ с возрастающей VC-размерностью. Идея заключается в том, что более сложные классы гипотез могут лучше аппроксимировать целевую функцию, но требуют больше данных для обобщения.

Алгоритм выбирает класс $\mathcal{H}_i$ и гипотезу $h \in \mathcal{H}_i$, минимизируя сумму эмпирического риска и штрафа за сложность:
\begin{equation}
\hat{h} = \arg\min_{i, h \in \mathcal{H}_i} \left[\hat{R}_S(h) + \text{penalty}(\text{VCdim}(\mathcal{H}_i), m)\right].
\end{equation}

Штраф за сложность обычно имеет вид $C\sqrt{d_i \log(m)/m}$, где $d_i$ --- VC-размерность класса $\mathcal{H}_i$, а $C$ --- константа, зависящая от желаемого уровня уверенности. Этот подход лежит в основе многих методов регуляризации, включая SVM с различными ядрами, где выбор ядра определяет сложность класса гипотез.

\paragraph{Ограничения VC-теории}

Однако VC-оценки часто оказываются слишком пессимистичными для практических приложений. Они не учитывают несколько важных факторов. Во-первых, они не учитывают специфику данных и распределения $P$. VC-размерность зависит только от класса гипотез, но не от того, как данные распределены в пространстве признаков. Во-вторых, они не учитывают структуру алгоритма обучения, а только класс гипотез. Разные алгоритмы, работающие с одним и тем же классом гипотез, могут иметь разную обобщающую способность. В-третьих, они не учитывают индуктивные смещения, заложенные в алгоритм. Например, алгоритм градиентного спуска может иметь неявное предпочтение к определённым типам решений, что не отражается в VC-размерности.

Более того, VC-оценки для глубоких нейронных сетей с миллионами параметров предсказывают необходимость астрономически больших выборок, что противоречит эмпирическим наблюдениям. Например, для сети с $10^6$ параметрами VC-оценки могут предсказывать необходимость выборки размером $10^{12}$ или больше, в то время как на практике такие сети успешно обучаются на выборках размером $10^5$ или меньше.

\paragraph{Альтернативные комбинаторные подходы}

Воронцов \cite{vorontsov2004combinatorial} в работе 2004 года развил альтернативный комбинаторный подход, основанный на скользящем контроле и учёте структуры семейства алгоритмов. Ключевая идея заключается в использовании функционалов качества, учитывающих не только сложность класса гипотез, но и специфику алгоритма обучения. Метод скользящего контроля позволяет получить более точные оценки обобщающей способности, учитывая реальное поведение алгоритма на данных. Однако и этот подход имеет ограничения применимости, особенно для сложных алгоритмов оптимизации.

Энтони и Бартлетт \cite{anthony1999neural} в монографии 1999 года систематизировали теорию обучения нейронных сетей, включая оценки VC-размерности для различных архитектур. Кирнс и Вазирани \cite{kearns1994introduction} в монографии 1994 года ввели основы вычислительной теории обучения, связав сложность обучения с вычислительной сложностью алгоритмов. Шалев-Шварц и Бен-Давид \cite{shalev2014understanding} в монографии 2014 года представили современное понимание машинного обучения от теории к алгоритмам, объединив различные теоретические подходы.

\paragraph{Резюме: Комбинаторные подходы}

VC-теория даёт универсальные теоретические гарантии обобщения через VC-размерность класса гипотез. Ключевой результат: для класса с VC-размерностью $d$ требуется выборка размера $O(d/\varepsilon)$ для обучения с точностью $\varepsilon$. Однако VC-оценки часто слишком пессимистичны для глубоких сетей (предсказывают выборки в 100-1000 раз больше необходимых), не учитывают распределение данных, структуру алгоритма и индуктивные смещения. Для простых моделей VC-теория остаётся практичным инструментом.

\subsubsection{Вероятностные меры сложности}

Вероятностные меры сложности представляют собой альтернативу комбинаторным подходам, учитывающую распределение данных и вероятностную структуру процесса обучения. В отличие от VC-размерности, которая зависит только от класса гипотез, вероятностные меры учитывают конкретную выборку данных, что позволяет получать более точные оценки обобщения.

\paragraph{Средние Радемахера и Гауссовская сложность}

Для класса функций $\mathcal{F} \subseteq \mathbb{R}^{\mathcal{X}}$ и выборки $S = \{x_1, \ldots, x_m\}$ средние Радемахера определяются как:
\begin{equation}
\hat{\mathfrak{R}}_S(\mathcal{F}) = \mathbb{E}_{\boldsymbol{\sigma}}\left[\sup_{f \in \mathcal{F}} \frac{1}{m}\sum_{i=1}^m \sigma_i f(x_i)\right],
\end{equation}
где $\sigma_1, \ldots, \sigma_m$ --- независимые случайные величины Радемахера: $\mathbb{P}(\sigma_i = +1) = \mathbb{P}(\sigma_i = -1) = 1/2$. 

Гауссовская сложность определяется аналогично, но с гауссовскими случайными величинами $g_i \sim \mathcal{N}(0,1)$ вместо $\sigma_i$:
\begin{equation}
\hat{\mathfrak{G}}_S(\mathcal{F}) = \mathbb{E}_{\boldsymbol{g}}\left[\sup_{f \in \mathcal{F}} \frac{1}{m}\sum_{i=1}^m g_i f(x_i)\right].
\end{equation}

Средние Радемахера и Гауссовская сложность связаны между собой: для ограниченных функций они отличаются не более чем на константу. Гауссовская сложность часто более удобна для теоретического анализа, так как гауссовские случайные величины обладают лучшими свойствами концентрации.

Бартлетт и Мендельсон \cite{bartlett2002rademacher} в работе 2002 года предложили использовать средние Радемахера как альтернативу VC-размерности. Однако их оценка требует предположения о доступности выборки $S$ для вычисления средних Радемахера, что часто нарушается на практике для больших моделей, где точное вычисление требует решения задачи оптимизации над всем классом функций. 

Средние Радемахера обладают преимуществами перед VC-размерностью: они учитывают распределение данных через выборку $S$, что позволяет получать более точные оценки для конкретных распределений; они дают более точные оценки для вещественнозначных функций, для которых VC-размерность не применима напрямую; они связаны с геометрией класса функций в пространстве признаков. Однако эти преимущества реализуются только при условии возможности точного вычисления средних Радемахера, что вычислительно сложно для глубоких сетей.

Ключевой результат связывает средние Радемахера с ошибкой обобщения. Для класса функций $\mathcal{F}$ с функциями, принимающими значения в $[0,1]$, с вероятностью не менее $1 - \delta$ для всех $f \in \mathcal{F}$ выполняется:
\begin{equation}
\mathbb{E}[f] \leq \frac{1}{m}\sum_{i=1}^m f(x_i) + 2\hat{\mathfrak{R}}_S(\mathcal{F}) + \sqrt{\frac{\log(1/\delta)}{2m}}.
\end{equation}

Эта оценка показывает, что средние Радемахера напрямую связаны с ошибкой обобщения, и чем меньше средние Радемахера, тем лучше обобщение.

Кольчинский \cite{koltchinskii2001rademacher} в работе 2001 года развил теорию средних Радемахера, показав их связь с PAC-обучением и получив более точные оценки обобщения. Он показал, что средние Радемахера могут быть использованы для получения адаптивных оценок обобщения, которые автоматически подстраиваются под сложность данных.

\paragraph{Метрические энтропии}

Метрическая $\varepsilon$-энтропия класса функций $\mathcal{F}$ относительно метрики $d$ определяется как логарифм минимального числа шаров радиуса $\varepsilon$, покрывающих $\mathcal{F}$:
\begin{equation}
H(\mathcal{F}, \varepsilon, d) = \log N(\mathcal{F}, \varepsilon, d),
\end{equation}
где $N(\mathcal{F}, \varepsilon, d)$ --- минимальное число шаров радиуса $\varepsilon$, покрывающих $\mathcal{F}$ в метрике $d$.

Классическая работа Колмогорова и Тихомирова \cite{kolmogorov1961epsilon}, опубликованная в 1961 году, установила связь между метрической энтропией и сложностью класса функций. Они показали, что метрическая энтропия является естественной мерой сложности функциональных классов и может быть использована для получения оценок аппроксимации и обобщения.

Метрическая энтропия связана со средними Радемахера через так называемую цепочку Радемахера. Для класса функций $\mathcal{F}$ с ограниченными функциями выполняется:
\begin{equation}
\hat{\mathfrak{R}}_S(\mathcal{F}) \leq C \int_0^{\text{diam}(\mathcal{F})} \sqrt{\frac{H(\mathcal{F}, \varepsilon, d)}{m}} d\varepsilon,
\end{equation}
где $\text{diam}(\mathcal{F})$ --- диаметр класса $\mathcal{F}$ в метрике $d$, а $C$ --- универсальная константа.

Дадли \cite{dudley1978central} в работе 1978 года исследовал центральные предельные теоремы для эмпирических мер, показав, как метрическая энтропия связана с асимптотическим поведением эмпирических процессов. Поллард \cite{pollard1984convergence} в монографии 1984 года систематизировал теорию сходимости стохастических процессов, включая равномерную сходимость эмпирических мер. Дадли \cite{dudley1999uniform} в монографии 1999 года развил теорию равномерных центральных предельных теорем, которые являются основой для получения вероятностных границ обобщения. Ван дер Ваарт и Веллнер \cite{van1996weak} в монографии 1996 года систематизировали теорию слабой сходимости и эмпирических процессов, показав, как метрическая энтропия связана с равномерной сходимостью.

\paragraph{Размерность Фэт-шеттеринга и псевдоразмерность}

Для вещественнозначных функций VC-размерность не применима напрямую, так как она определена только для бинарных классификаторов. Вместо этого используются обобщения VC-размерности, такие как размерность Фэт-шеттеринга и псевдоразмерность.

Множество точек $\{x_1, \ldots, x_k\} \subseteq \mathcal{X}$ называется $\gamma$-разбиваемым классом $\mathcal{F}$, если существуют числа $r_1, \ldots, r_k$ такие, что для любого вектора $(y_1, \ldots, y_k) \in \{\pm 1\}^k$ найдётся функция $f \in \mathcal{F}$ с $f(x_i) \geq r_i + \gamma$ при $y_i = +1$ и $f(x_i) \leq r_i - \gamma$ при $y_i = -1$. Размерность Фэт-шеттеринга $\text{fat}_{\gamma}(\mathcal{F})$ определяется как максимальное $k$, для которого существует $\gamma$-разбиваемое множество из $k$ точек.

Псевдоразмерность $\text{Pdim}(\mathcal{F})$ определяется как VC-размерность класса функций $\{x \mapsto \text{sign}(f(x) - t) : f \in \mathcal{F}, t \in \mathbb{R}\}$. Псевдоразмерность связана с размерностью Фэт-шеттеринга: если $\text{fat}_{\gamma}(\mathcal{F}) < \infty$ для некоторого $\gamma > 0$, то $\text{Pdim}(\mathcal{F}) < \infty$.

Размерность Фэт-шеттеринга позволяет получать оценки обобщения для вещественнозначных функций, аналогичные оценкам VC-теории для бинарных классификаторов. Для класса функций $\mathcal{F}$ с ограниченными функциями и конечной размерностью Фэт-шеттеринга можно получить вероятностные границы обобщения, зависящие от $\text{fat}_{\gamma}(\mathcal{F})$ и размера выборки.

\paragraph{Ограничения вероятностных мер}

Леду \cite{ledoux2001concentration} в монографии 2001 года систематизировал методы концентрации меры, которые лежат в основе многих вероятностных оценок обобщения. Однако вычисление средних Радемахера и метрических энтропий для больших моделей может быть вычислительно сложным, что ограничивает их практическое применение. 

Для глубоких нейронных сетей точное вычисление средних Радемахера требует решения задачи оптимизации над всем классом функций, что вычислительно неразрешимо. Приближённые методы, такие как стохастическая оптимизация или выборка подмножества функций, могут давать неточные оценки. Аналогично, вычисление метрической энтропии для больших классов функций является сложной задачей, требующей знания структуры класса функций.

Несмотря на эти ограничения, вероятностные меры сложности остаются важным инструментом теоретического анализа обобщения, особенно для вещественнозначных функций и случаев, когда распределение данных известно или может быть оценено.

\paragraph{Резюме: Вероятностные меры сложности}

Вероятностные меры (средние Радемахера, метрические энтропии) учитывают распределение данных и дают более точные оценки, чем VC-размерность. Средние Радемахера связаны с геометрией класса функций и дают более оптимистичные оценки для вещественнозначных функций. Однако вычисление этих мер для больших моделей вычислительно сложно, что ограничивает практическое применение. Для глубоких сетей точное вычисление требует решения задачи оптимизации над всем классом функций, что неразрешимо.

\subsubsection{Информационно-теоретические подходы}

Информационно-теоретические подходы к оценке сложности моделей основаны на идее, что обобщающая способность алгоритма связана с количеством информации, которое алгоритм извлекает из обучающей выборки. Интуиция заключается в том, что алгоритмы, которые «запоминают» меньше информации о конкретных примерах обучающей выборки, должны лучше обобщаться на новые данные.

\paragraph{Взаимная информация и обобщение}

Руссо и Цзоу \cite{russo2016controlling} в работе 2016 года впервые установили формальную связь между взаимной информацией $I(S; A(S))$ обучающей выборки $S$ и алгоритма $A(S)$ и ошибкой обобщения. Однако их подход требует знания распределения $P(A(S)|S)$, которое для сложных алгоритмов неизвестно. Взаимная информация измеряет количество информации, которое алгоритм $A$ извлекает из выборки $S$:

\begin{equation}
I(S; A(S)) = H(S) - H(S|A(S)) = H(A(S)) - H(A(S)|S),
\end{equation}
где $H(\cdot)$ --- энтропия Шеннона, а $H(\cdot|\cdot)$ --- условная энтропия.

Интуиция этого подхода заключается в следующем: если алгоритм извлекает много информации из обучающей выборки, то он может переобучиться на конкретные примеры, что приведёт к плохому обобщению. Наоборот, если алгоритм извлекает мало информации, то он не может запомнить конкретные примеры и должен обобщаться лучше.

Сюй и Рагинский \cite{xu2017information} в 2017 году развили этот подход, доказав общие границы обобщения через взаимную информацию:
\begin{equation}
\mathbb{E}[R(A(S)) - \hat{R}_S(A(S))] \leq \sqrt{\frac{2I(S; A(S))}{m}}.
\end{equation}

Эта граница показывает, что ожидаемая ошибка обобщения ограничена сверху квадратным корнем из взаимной информации, делённой на размер выборки. Однако критический анализ показывает, что для глубоких нейронных сетей взаимная информация $I(S; A(S))$ может быть очень большой (порядка размера выборки), что приводит к тривиальным границам обобщения. Более того, вычисление взаимной информации требует знания распределения $P(A(S)|S)$, которое для сложных алгоритмов неизвестно и может быть очень сложным. Таким образом, хотя граница является универсальной и применима к любому алгоритму обучения, её практическая применимость ограничена невозможностью точного вычисления взаимной информации.

\paragraph{Информационная бутылочка}

Развитие подхода привело к использованию концепции информационной бутылочки (information bottleneck) Тишби и др. \cite{tishby1999information}, предложенной в 1999 году. Информационная бутылочка представляет собой принцип, согласно которому процесс обучения можно рассматривать как сжатие информации о данных с сохранением только релевантной информации для задачи.

Формально, информационная бутылочка ищет представление $T$ данных $X$, которое минимизирует:
\begin{equation}
\mathcal{L}_{\text{IB}} = I(X; T) - \beta I(T; Y),
\end{equation}
где $I(X; T)$ --- взаимная информация между данными и представлением (измеряет сжатие), $I(T; Y)$ --- взаимная информация между представлением и целевой переменной (измеряет релевантность), а $\beta$ --- параметр, контролирующий компромисс между сжатием и релевантностью.

Шамир и др. \cite{shamir2010learnability} в работе 2010 года связали информационную бутылочку с обобщающей способностью, показав, что процесс обучения можно рассматривать как оптимизацию информационной бутылочки. Они показали, что хорошее обобщение связано с нахождением представлений, которые сжимают информацию о данных, сохраняя при этом информацию, необходимую для предсказания.

\paragraph{Применение к стохастическим алгоритмам}

Информационно-теоретический подход особенно эффективен для стохастических алгоритмов, таких как стохастический градиентный спуск Ланжевена (SGLD), где шум в алгоритме естественным образом ограничивает взаимную информацию. Негреа и др. \cite{negrea2019information} в работе 2019 года получили информационно-теоретические границы обобщения для SGLD через оценки, зависящие от данных. Они показали, что шум в SGLD ограничивает количество информации, которое алгоритм может извлечь из данных, что обеспечивает хорошее обобщение.

Бу и др. \cite{li2020understanding} в работе 2020 года исследовали обобщение в глубоком обучении через тензорные методы, показав, как структура данных влияет на взаимную информацию и обобщающую способность.

\paragraph{Ограничения информационно-теоретического подхода}

Однако вычисление взаимной информации для сложных алгоритмов является нетривиальной задачей, что существенно ограничивает практическое применение этого подхода. Взаимная информация между выборкой $S$ и алгоритмом $A(S)$ требует знания распределения $P(A(S)|S)$, которое для сложных алгоритмов, таких как глубокие нейронные сети, неизвестно и может быть очень сложным.

Даже для простых алгоритмов точное вычисление взаимной информации может быть вычислительно сложным. Приближённые методы, такие как оценка через вариационные границы или методы Монте-Карло, могут давать неточные оценки, что ограничивает практическую применимость подхода.

Кроме того, информационно-теоретические границы часто оказываются слишком слабыми для получения нетривиальных оценок обобщения. Для глубоких нейронных сетей взаимная информация $I(S; A(S))$ может быть очень большой (порядка размера выборки), что приводит к тривиальным границам обобщения. Это связано с тем, что алгоритм может извлекать много информации о данных, но при этом всё равно хорошо обобщаться благодаря структуре данных или индуктивным смещениям алгоритма.

Несмотря на эти ограничения, информационно-теоретический подход предоставляет важную интуицию о связи между информацией и обобщением и может быть полезен для понимания механизмов обобщения в определённых классах алгоритмов, особенно стохастических.

\paragraph{Резюме: Информационно-теоретические подходы}

Информационно-теоретический подход связывает обобщающую способность с количеством информации, которое алгоритм извлекает из обучающей выборки. Ключевая идея: меньше информации --- лучше обобщение. Границы обобщения зависят от взаимной информации $I(S; A(S))$. Однако вычисление взаимной информации для сложных алгоритмов требует знания распределения $P(A(S)|S)$, которое неизвестно. Для глубоких сетей взаимная информация может быть очень большой, что приводит к тривиальным границам. Подход эффективен для стохастических алгоритмов, где шум естественным образом ограничивает взаимную информацию.

\subsubsection{PAC-Байесовские методы}

PAC-Байесовский подход объединяет байесовскую теорию с PAC-теорией, позволяя получать вероятностные границы обобщения для байесовских алгоритмов. Этот подход был разработан Мак-Аллестером \cite{mcallester1999pac} в работе 1999 года и стал важным инструментом для получения нетривиальных границ обобщения для сложных моделей, включая глубокие нейронные сети.

\paragraph{Основные PAC-Байесовские неравенства}

В PAC-Байесовском подходе мы рассматриваем распределение $Q$ над гипотезами, а не отдельную гипотезу. Байесовский риск определяется как:
\begin{equation}
R(Q) = \mathbb{E}_{h \sim Q}[R(h)],
\end{equation}
а эмпирический байесовский риск --- как:
\begin{equation}
\hat{R}_S(Q) = \mathbb{E}_{h \sim Q}[\hat{R}_S(h)].
\end{equation}

Основное PAC-Байесовское неравенство связывает байесовский риск с эмпирическим риском через KL-дивергенцию между априорным распределением $P$ и апостериорным распределением $Q$:
\begin{equation}
R(Q) \leq \hat{R}_S(Q) + \sqrt{\frac{\text{KL}(Q\|P) + \log(2m/\delta)}{2(m-1)}},
\end{equation}
где $\text{KL}(Q\|P) = \mathbb{E}_{h \sim Q}[\log(Q(h)/P(h))]$ --- KL-дивергенция между распределениями $Q$ и $P$.

Эта граница показывает, что ошибка обобщения ограничена сверху квадратным корнем из суммы KL-дивергенции и логарифмического члена, делённой на размер выборки. Чем ближе апостериорное распределение $Q$ к априорному $P$ (то есть чем меньше KL-дивергенция), тем лучше граница обобщения. Это отражает байесовский принцип: если апостериорное распределение сильно отличается от априорного, это означает, что алгоритм извлёк много информации из данных, что может привести к переобучению.

\paragraph{Вычисление непустых границ для глубоких сетей}

Дзюгайте и Рой \cite{dziugaite2017computing} в работе 2017 года впервые вычислили непустые (non-vacuous) PAC-Байесовские границы для глубоких нейронных сетей. До этой работы PAC-Байесовские границы для глубоких сетей были тривиальными (больше 1) или требовали нереалистичных предположений. Однако критический анализ показывает, что получение нетривиальных границ требует тщательного подбора априорного распределения, который часто основан на знании структуры данных или архитектуры модели, что может привести к оптимистичным оценкам. Более того, априорное распределение должно быть выбрано до наблюдения данных, но на практике часто выбирается постфактум, что нарушает принципы PAC-Байесовского подхода.

Ключевая идея заключается в выборе априорного распределения, которое отражает структуру обученной сети. Например, априорное распределение может быть сосредоточено вокруг случайно инициализированной сети, а апостериорное --- вокруг обученной сети. KL-дивергенция между этими распределениями измеряет, насколько обучение изменило распределение весов по сравнению с инициализацией.

\paragraph{Стохастический вариационный вывод}

Хоффман и др. \cite{hoffman2013stochastic} в работе 2013 года предложили стохастический вариационный вывод (SVI) как эффективный метод приближённого байесовского вывода. SVI позволяет применять PAC-Байесовские методы к большим моделям и выборкам, для которых точный байесовский вывод вычислительно неразрешим.

В SVI апостериорное распределение $Q$ аппроксимируется параметрическим семейством распределений $Q_\phi$, и параметры $\phi$ оптимизируются для минимизации KL-дивергенции $\text{KL}(Q_\phi\|P)$ при условии хорошего эмпирического риска. Это позволяет получать PAC-Байесовские границы для больших моделей, обучая параметрическое приближение апостериорного распределения.

\paragraph{Развитие PAC-Байесовской теории}

Зигер \cite{seeger2002pac} в работе 2002 года развил PAC-Байесовский подход для гауссовских процессов, показав, как можно получить границы обобщения для байесовских методов непараметрической регрессии. Катони \cite{catoni2007pac} в монографии 2007 года систематизировал PAC-Байесовскую теорию, развив более общие неравенства и показав их связь с классической байесовской теорией.

Современные исследования направлены на улучшение PAC-Байесовских границ для глубоких сетей. Одним из направлений является разработка более точных оценок KL-дивергенции для конкретных архитектур сетей. Другим направлением является использование структурированных априорных распределений, которые отражают иерархическую структуру глубоких сетей.

\paragraph{Ограничения PAC-Байесовского подхода}

Однако выбор априорного распределения может существенно влиять на качество оценок, и для получения нетривиальных границ требуется тщательный подбор априорных распределений, что не всегда возможно. Априорное распределение должно быть выбрано до наблюдения данных, но на практике часто выбирается на основе знания о структуре данных или архитектуре модели, что может привести к оптимистичным оценкам.

Кроме того, PAC-Байесовские границы зависят от KL-дивергенции между априорным и апостериорным распределениями, которая может быть большой для сложных моделей. Для глубоких нейронных сетей KL-дивергенция может быть порядка числа параметров, что приводит к необходимости больших выборок для получения нетривиальных границ.

Несмотря на эти ограничения, PAC-Байесовский подход остаётся одним из немногих методов, позволяющих получать нетривиальные теоретические границы обобщения для глубоких нейронных сетей, что делает его важным инструментом теоретического анализа.

\paragraph{Резюме: PAC-Байесовские методы}

PAC-Байесовский подход объединяет байесовскую теорию с PAC-теорией, позволяя получать вероятностные границы обобщения через KL-дивергенцию между априорным и апостериорным распределениями. Ключевое преимущество: можно получить нетривиальные границы для глубоких сетей при тщательном подборе априорного распределения. Однако выбор априора остаётся произвольным и может существенно влиять на качество оценок. Для глубоких сетей KL-дивергенция может быть порядка числа параметров, что требует больших выборок. Подход эффективен, когда априорное распределение отражает структуру обученной сети.

\subsection{Эмпирические методы оценки сложности}

\subsubsection{Современные архитектуры: трансформеры и диффузионные модели}

Современные архитектуры, такие как трансформеры и диффузионные модели, стали доминирующими в машинном обучении, но теоретическое понимание их обобщающей способности остаётся неполным. В данном подразделе рассматриваются попытки объяснить успех этих архитектур с точки зрения теории обобщения.

\paragraph{Трансформеры и механизм внимания}

Васавани и др. \cite{vaswani2017attention} в работе 2017 года предложили архитектуру трансформера с механизмом внимания, который стал основой для современных языковых моделей. Ключевой компонент трансформера --- механизм самовнимания (self-attention), который позволяет модели фокусироваться на релевантных частях входной последовательности.

Теоретический анализ механизма внимания показывает, что он может быть интерпретирован как неявная регуляризация. Донг и др. \cite{dong2021attention} в работе 2021 года показали, что для чистого внимания (pure attention, без residual connections и layer normalization) ранг attention-матрицы теряется двойно экспоненциально с глубиной: для трансформера глубины $L$ ранг attention-матрицы убывает как $O(\exp(-cL))$ для некоторой константы $c$. Важно отметить, что этот результат относится именно к чистому вниманию без дополнительных компонентов; в практических трансформерах с residual connections и layer normalization этот эффект может быть смягчён. Это ограничивает выразительную способность модели, но также может способствовать обобщению, действуя как неявная регуляризация. Однако это также может привести к проблемам с обучением глубоких трансформеров, что объясняет необходимость residual connections и layer normalization.

Существуют конкурирующие гипотезы о механизме обобщения трансформеров. Одна из них связывает обобщение с теорией нейрального касательного ядра (neural tangent kernel, NTK) \cite{jacot2018neural}. В режиме NTK трансформер может быть аппроксимирован ядерным методом, что даёт теоретические гарантии обобщения. Однако Жако и др. показали, что трансформеры в режиме NTK имеют ограниченную выразительную способность и не могут объяснить их успех на практических задачах.

Другая гипотеза связывает обобщение с implicit bias оптимизации. Судри и др. \cite{soudry2018implicit} показали, что градиентный спуск на разделимых данных имеет неявное предпочтение к решениям с максимальным запасом (maximum margin). Для трансформеров это может означать, что оптимизация неявно выбирает решения с определённой структурой, способствующей обобщению.

Третья гипотеза связывает обобщение с индуктивными смещениями архитектуры. Трансформеры имеют архитектурные особенности (positional encoding, multi-head attention, layer normalization), которые могут создавать индуктивные смещения, способствующие обобщению на языковых задачах. Однако теоретическое понимание этих смещений остаётся неполным.

Критический анализ показывает, что ни одна из этих гипотез не может полностью объяснить обобщающую способность трансформеров. Теория NTK не применима к практическим трансформерам, которые находятся в режиме feature learning, а не kernel regime. Implicit bias оптимизации может объяснить обобщение для простых задач, но не для сложных языковых задач. Индуктивные смещения архитектуры остаются спекулятивными без теоретического обоснования.

Существуют альтернативные гипотезы, которые заслуживают дальнейшего исследования. Одна из них связывает обобщение трансформеров с их способностью к компрессии последовательностей через механизм внимания. Другая гипотеза рассматривает трансформеры как универсальные аппроксиматоры последовательностей с определённой структурой, что может объяснить их успех на языковых задачах. Третья гипотеза связывает обобщение с динамикой обучения: трансформеры могут проходить через последовательность представлений, которые постепенно становятся более обобщающими. Однако все эти гипотезы требуют дальнейшего теоретического и эмпирического исследования.

\paragraph{Диффузионные модели}

Хо и др. \cite{ho2020denoising} в работе 2020 года предложили диффузионные вероятностные модели для денойзинга. Сонг и др. \cite{song2021score} в работе 2021 года связали генеративное моделирование на основе скоринга со стохастическими дифференциальными уравнениями, показав, что процесс диффузии может быть описан как решение стохастического дифференциального уравнения.

Теоретический анализ обобщающей способности диффузионных моделей находится в начальной стадии, но существуют некоторые теоретические зачатки. Ключевые вопросы включают:

\begin{itemize}
\item Как процесс диффузии влияет на обобщающую способность модели? Процесс диффузии добавляет шум к данным, что может действовать как регуляризация, но теоретическое обоснование этого эффекта отсутствует.
\item Какая связь между числом шагов диффузии $T$ и обобщением? Эмпирически наблюдается, что большее число шагов улучшает качество генерации, но ухудшает обобщение на новые данные. Теоретическое объяснение этого компромисса отсутствует.
\item Как архитектура модели (например, U-Net) влияет на обобщение? U-Net имеет архитектурные особенности (skip connections, downsampling/upsampling), которые могут создавать индуктивные смещения, но их влияние на обобщение не изучено теоретически.
\end{itemize}

Эмпирически наблюдается, что диффузионные модели хорошо обобщаются на генеративных задачах, но теоретическое объяснение этого явления отсутствует. В отличие от трансформеров, для диффузионных моделей практически нет работ, связывающих их архитектуру или процесс обучения с обобщающей способностью. Это указывает на необходимость разработки новых теоретических подходов, специфичных для диффузионных моделей.

\paragraph{Критический анализ теоретических попыток}

Утверждение "теоретические методы оценки сложности отсутствуют" для трансформеров и диффузионных моделей является неточным. Существуют работы по анализу expressive power трансформеров, их dynamics при обучении, и приближению трансформеров ядерными методами. Однако эти работы не дают полного объяснения обобщающей способности.

Для трансформеров работы по rank collapse \cite{dong2021attention} показывают ограничения выразительной способности, но не объясняют, почему это способствует обобщению. Работы по NTK показывают, что трансформеры в режиме ядра имеют теоретические гарантии, но практические трансформеры находятся в режиме feature learning. Работы по implicit bias показывают предпочтения оптимизации, но не объясняют обобщение на сложных задачах.

Для диффузионных моделей теоретические работы практически отсутствуют. Существуют работы по анализу процесса диффузии как стохастического дифференциального уравнения, но они не связывают это с обобщающей способностью.

\paragraph{Открытые проблемы}

Вопросы о том, почему attention-механизмы эффективны, как диффузионные модели обобщаются, и каковы оптимальные соотношения между размером модели и объёмом данных для этих архитектур, остаются открытыми. Разработка теоретического понимания для трансформеров и диффузионных моделей является одной из наиболее актуальных задач современной теории машинного обучения, требующей новых теоретических подходов, учитывающих специфику этих архитектур и dynamics их обучения.

\subsubsection{Ландшафтные меры и геометрические подходы}

Ландшафтные меры сложности основаны на анализе геометрии функции потерь в окрестности найденного минимума. Идея заключается в том, что форма минимума (плоский или острый) связана с обобщающей способностью модели. Плоские минимумы, где функция потерь изменяется медленно в широкой области, должны обеспечивать лучшую устойчивость к малым изменениям в данных и, следовательно, лучшее обобщение.

\paragraph{Визуализация ландшафта функции потерь}

Ли и др. \cite{li2018visualizing} в работе 2018 года предложили методы визуализации ландшафта функции потерь нейронных сетей, используя проекции на низкоразмерные подпространства. Они обнаружили, что успешные модели часто находятся в широких, плоских минимумах, в то время как неудачные --- в узких, острых минимумах. 

Метод визуализации заключается в следующем. Выбираются два случайных направления $d_1$ и $d_2$ в пространстве параметров, и функция потерь визуализируется на плоскости, натянутой на эти направления: $L(\theta + \alpha d_1 + \beta d_2)$, где $\theta$ --- найденный минимум, а $\alpha$ и $\beta$ --- координаты на плоскости. Визуализация показывает форму минимума: плоские минимумы выглядят как широкие долины, а острые --- как узкие впадины.

Хохрейтер и Шмидхубер \cite{hochreiter1997flat} в работе 1997 года впервые выдвинули гипотезу о том, что плоские минимумы обобщаются лучше, чем острые. Интуиция заключается в том, что если минимум плоский, то небольшие изменения в параметрах не сильно изменяют значение функции потерь, что должно обеспечивать устойчивость к вариациям в данных.

\paragraph{Гессиан и обобщение}

Киселёв и Грабовой \cite{kiselev2024unraveling} в работе 2024 года провели теоретический анализ связи между гессианом функции потерь и способностью к обобщению. Гессиан $H = \nabla^2 L(\theta)$ в точке минимума $\theta$ характеризует локальную кривизну функции потерь. 

Собственные значения гессиана показывают, насколько быстро функция потерь изменяется в различных направлениях. Большие собственные значения соответствуют направлениям с высокой кривизной (острые минимумы), а малые --- направлениям с низкой кривизной (плоские минимумы). 

Меры сложности, основанные на гессиане, включают:
\begin{itemize}
\item След гессиана $\text{tr}(H) = \sum_i \lambda_i$, где $\lambda_i$ --- собственные значения. Большой след указывает на острый минимум.
\item Определитель гессиана $\det(H) = \prod_i \lambda_i$. Малый определитель указывает на наличие направлений с малой кривизной.
\item Число малых собственных значений. Большое число малых собственных значений указывает на плоский минимум.
\item Норма гессиана $\|H\|_F = \sqrt{\sum_i \lambda_i^2}$.
\end{itemize}

Сагун и др. \cite{sagun2017eigenvalues} в работе 2017 года исследовали собственные значения гессиана в глубоких сетях, показав их связь с обобщением. Они обнаружили, что в успешных моделях большинство собственных значений гессиана близки к нулю, что указывает на плоские минимумы.

\paragraph{Алгоритмы поиска плоских минимумов}

Чодхури и др. \cite{chaudhari2019entropy} в работе 2019 года развили идею плоских минимумов, предложив алгоритм Entropy-SGD, явно ищущий плоские минимумы. Алгоритм минимизирует не саму функцию потерь, а её сглаженную версию, которая предпочитает плоские минимумы. Это достигается путём добавления энтропийного члена, который штрафует острые минимумы.

Пеннингтон и др. \cite{pennington2017resurrecting} в работе 2017 года связали динамическую изометрию с обобщающей способностью. Динамическая изометрия означает, что градиенты в процессе обучения остаются примерно одинаковыми по норме, что обеспечивает стабильность обучения и, возможно, лучшую обобщающую способность.

\paragraph{Ограничения ландшафтных мер}

Однако вычисление гессиана для больших сетей является вычислительно сложной задачей, что ограничивает практическое применение этих мер. Гессиан имеет размер $n \times n$, где $n$ --- число параметров, что для глубоких сетей может быть миллионами или миллиардами. Точное вычисление гессиана требует $O(n^2)$ памяти и вычислений, что непрактично для больших моделей.

Приближённые методы, такие как вычисление только нескольких наибольших собственных значений или использование стохастических оценок, могут давать неточные результаты. Кроме того, связь между геометрией минимума и обобщением не всегда является причинно-следственной. Плоские минимумы могут коррелировать с хорошим обобщением, но не обязательно его вызывают. Возможно, что и плоские минимумы, и хорошее обобщение являются следствиями какого-то третьего фактора, такого как структура данных или индуктивные смещения алгоритма.

Несмотря на эти ограничения, ландшафтные меры предоставляют важную интуицию о связи между геометрией оптимизации и обобщением и могут быть полезны для понимания механизмов обобщения в определённых случаях.

\paragraph{Резюме: Ландшафтные меры}

Ландшафтные меры связывают геометрию функции потерь (плоские vs. острые минимумы) с обобщающей способностью. Ключевые инсайты: успешные модели часто находятся в плоских минимумах; собственные значения гессиана могут характеризовать форму минимума. Однако вычисление гессиана для больших сетей вычислительно сложно, и причинно-следственная связь между геометрией и обобщением не установлена. Эти меры дают качественную интуицию, но не дают количественных оценок сложности, применимых на практике.

\subsubsection{Двойной спуск и режимы обучения}

Явление двойного спуска представляет собой фундаментальный вызов классической теории обобщения. Белкин и др. \cite{belkin2019reconciling} в работе 2019 года примирили современную практику машинного обучения с классическим компромиссом смещения и дисперсии, обнаружив явление двойного спуска. Наккиран и др. \cite{nakkiran2021deep} в работе 2021 года исследовали глубокий двойной спуск, показав, что большие модели и больше данных могут ухудшать производительность.

\paragraph{Феномен двойного спуска}

Двойной спуск демонстрирует, что при увеличении сложности модели (числа параметров) ошибка обобщения сначала уменьшается (классический режим), затем увеличивается (переобучение), и снова уменьшается (современный режим) (см. рисунок~\ref{fig:double_descent}). Это противоречит классической теории, предсказывающей монотонное ухудшение обобщения с ростом сложности. Важно отметить, что явление наиболее отчётливо наблюдается в определённых режимах (конечные модели, обученные с помощью SGD) и на специфических задачах (регрессия, некоторые классификационные датасеты), и не является универсальным для всех типов моделей и задач.

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={Число параметров (логарифмическая шкала)},
    ylabel={Ошибка обобщения},
    width=12cm,
    height=8cm,
    grid=major,
    legend pos=south east,
    xmin=0, xmax=10,
    ymin=0, ymax=1.2
]
% Классическая теория (монотонное ухудшение)
\addplot[domain=0:10, samples=100, color=red, thick, dashed] {0.3 + 0.7 * exp(-x/3)};
\addlegendentry{Классическая теория (монотонное ухудшение)};

% Двойной спуск
\addplot[domain=0:3, samples=100, color=blue, thick] {0.4 - 0.2*x + 0.1*x^2};
\addplot[domain=3:6, samples=100, color=blue, thick] {0.1 + 0.3*(x-3) - 0.05*(x-3)^2};
\addplot[domain=6:10, samples=100, color=blue, thick] {0.4 - 0.15*(x-6) + 0.02*(x-6)^2};
\addlegendentry{Двойной спуск (эмпирическое наблюдение)};

% Вертикальная линия для точки перехода
\draw[dashed, gray] (axis cs:3,0) -- (axis cs:3,1.2);
\node[above] at (axis cs:3,1.1) {Переходный режим};

% Аннотации
\node[above] at (axis cs:1.5,0.2) {Классический режим};
\node[above] at (axis cs:4.5,0.8) {Переобучение};
\node[above] at (axis cs:8,0.3) {Современный режим};
\end{axis}
\end{tikzpicture}
\caption{Феномен двойного спуска: зависимость ошибки обобщения от числа параметров модели. Классическая теория предсказывает монотонное ухудшение, в то время как эмпирически наблюдается двойной спуск с переходом между режимами обучения.}
\label{fig:double_descent}
\end{figure}

Критический анализ показывает, что двойной спуск связан с переходом между двумя режимами обучения: режимом ядра (kernel regime) и режимом обучения признаков (feature learning regime). В режиме ядра модель ведёт себя как ядерный метод, и применима теория нейрального касательного ядра \cite{jacot2018neural}. В режиме обучения признаков модель активно изменяет свои представления, и теория NTK не применима.

\paragraph{Режим ядра vs. режим обучения признаков}

Жако и др. \cite{jacot2018neural} в работе 2018 года ввели концепцию нейрального касательного ядра (NTK). В режиме ядра, когда модель достаточно широкая, градиенты остаются примерно постоянными во время обучения, и модель может быть аппроксимирована ядерным методом. В этом режиме применимы теоретические гарантии обобщения для ядерных методов.

Однако практические глубокие сети находятся в режиме обучения признаков, где градиенты существенно изменяются во время обучения, и модель активно изменяет свои представления. В этом режиме теория NTK не применима, и теоретические гарантии обобщения отсутствуют.

Арора и др. \cite{arora2019fine} в работе 2019 года провели детальный анализ оптимизации и обобщения для перепараметризованных двухслойных нейронных сетей, показав переход между режимами. Шиза и др. \cite{chizat2019lazy} в работе 2019 года исследовали ленивое обучение в дифференцируемом программировании, показав условия, при которых модель находится в режиме ядра. Вудворт и др. \cite{woodworth2020kernel} в работе 2020 года исследовали режимы ядра и богатые режимы в перепараметризованных моделях.

\paragraph{Связь с обобщением}

Критический вопрос: какой режим обеспечивает лучшее обобщение? Эмпирически наблюдается, что модели в режиме обучения признаков часто обобщаются лучше, чем в режиме ядра, но теоретическое объяснение этого явления отсутствует. Возможно, режим обучения признаков позволяет модели лучше использовать структуру данных, что способствует обобщению.

Однако это также может приводить к переобучению, если модель слишком активно изменяет свои представления. Понимание баланса между режимами и его влияние на обобщение является важной открытой проблемой.

\subsubsection{Законы масштабирования}

Законы масштабирования представляют собой эмпирически обнаруженные закономерности, связывающие производительность модели с её размером, объёмом данных и вычислительными ресурсами. Эти законы стали важным инструментом для планирования экспериментов и прогнозирования производительности больших моделей, хотя их теоретическое обоснование остаётся неполным.

\paragraph{Эмпирическое открытие законов масштабирования}

Хестнесс и др. \cite{hestness2017deep} в работе 2017 года эмпирически обнаружили предсказуемые законы масштабирования для глубокого обучения. Они утверждают, что производительность модели (измеряемая, например, точностью на тестовой выборке) масштабируется как степенная функция от размера модели $N$, объёма данных $D$ и вычислительных ресурсов $C$. Однако критический анализ показывает, что эти законы справедливы только в определённых диапазонах размеров моделей и данных, и нарушаются при переходе к очень большим моделям или при изменении архитектуры:

\begin{equation}
\text{Performance} \propto N^{\alpha_N} D^{\alpha_D} C^{\alpha_C},
\end{equation}
где $\alpha_N$, $\alpha_D$ и $\alpha_C$ --- эмпирически определяемые показатели степени, обычно лежащие в диапазоне от 0.1 до 0.5.

Это означает, что для улучшения производительности на определённую величину необходимо увеличить размер модели, объём данных или вычислительные ресурсы на соответствующую величину. Например, если $\alpha_N = 0.3$, то для улучшения производительности в 2 раза необходимо увеличить размер модели примерно в $2^{1/0.3} \approx 10$ раз.

Каплан и др. \cite{kaplan2020scaling} в работе 2020 года установили законы масштабирования для языковых моделей, показав, что производительность масштабируется как степенная функция от размера модели и объёма данных. Они обнаружили, что для языковых моделей показатели степени составляют примерно $\alpha_N \approx 0.076$ и $\alpha_D \approx 0.095$, что означает, что увеличение размера данных более эффективно, чем увеличение размера модели.

\paragraph{Оптимальное соотношение размера модели и данных}

Хоффман и др. \cite{hoffman2022training} в работе 2022 года о Chinchilla показали, что для оптимального обучения необходимо балансировать размер модели и объём данных. Они обнаружили, что для языковых моделей оптимальное соотношение составляет примерно 20 токенов данных на параметр модели. Это означает, что для модели с $10^9$ параметрами оптимальный объём данных составляет примерно $2 \times 10^{10}$ токенов.

Это открытие противоречит предыдущим практикам, где часто использовались модели с большим числом параметров относительно объёма данных. Оно показывает, что для достижения оптимальной производительности необходимо тщательно балансировать размер модели и объём данных, а не просто увеличивать размер модели.

Хениган и др. \cite{henighan2020scaling} в работе 2020 года расширили законы масштабирования на авторегрессивные генеративные модели, показав, что производительность таких моделей также следует степенным законам.

\paragraph{Теоретическое обоснование законов масштабирования}

Однако эти законы носят эмпирический характер и не имеют полного теоретического обоснования. Вопрос о том, почему эти законы работают и при каких условиях они нарушаются, остаётся открытым. 

Одной из попыток теоретического обоснования является связь с теорией аппроксимации. Если целевая функция имеет определённую гладкость или структуру, то для её аппроксимации с заданной точностью требуется модель определённого размера и определённый объём данных. Степенные законы могут отражать эту связь между сложностью целевой функции и требованиями к модели и данным.

Другой возможной причиной является связь с информационной теорией. Если данные содержат определённое количество информации, то для её извлечения требуется модель определённого размера и определённый объём данных. Степенные законы могут отражать эту связь между информационным содержанием данных и требованиями к модели.

Однако эти объяснения остаются спекулятивными, и строгое теоретическое обоснование законов масштабирования остаётся открытой проблемой.

\paragraph{Ограничения законов масштабирования}

Кроме того, законы масштабирования не объясняют механизм обобщения, а лишь описывают эмпирические закономерности. Они не говорят, почему большие модели обобщаются лучше, а только то, что они это делают при определённых условиях. 

Законы масштабирования также могут нарушаться при переходе к очень большим моделям или при изменении архитектуры. Например, переход от свёрточных сетей к трансформерам может изменить показатели степени в законах масштабирования. Аналогично, законы масштабирования могут не работать для задач с очень специфической структурой или для моделей с определёнными архитектурными особенностями.

Несмотря на эти ограничения, законы масштабирования остаются важным эмпирическим инструментом для планирования экспериментов и прогнозирования производительности больших моделей, даже если их теоретическое обоснование остаётся неполным.

\paragraph{Резюме: Законы масштабирования}

Законы масштабирования описывают эмпирические закономерности, связывающие производительность модели с её размером, объёмом данных и вычислительными ресурсами через степенные зависимости. Ключевые показатели: $\alpha_N \approx 0.076$ (размер модели), $\alpha_D \approx 0.095$ (объём данных), $\alpha_C \approx 0.057$ (вычислительные ресурсы) для языковых моделей. Законы успешно используются для планирования экспериментов, но не имеют полного теоретического обоснования. Они могут нарушаться при переходе к очень большим моделям или при изменении архитектуры. Законы описывают, но не объясняют механизм обобщения.

\subsection{Связь оптимизации и обобщения}

\subsubsection{Устойчивость алгоритмов}

Теория устойчивости связывает способность алгоритма к обобщению с его устойчивостью к малым изменениям в обучающей выборке. Интуиция заключается в том, что алгоритмы, которые дают похожие результаты на похожих выборках, должны лучше обобщаться, так как они не переобучаются на конкретные примеры.

\paragraph{Формальное определение устойчивости}

Буске и Элиссефф \cite{bousquet2002stability} в работе 2002 года ввели формальное определение устойчивости алгоритма обучения. Алгоритм $A$ называется $\beta$-устойчивым по функции потерь, если для любых выборок $S$ и $S'$, отличающихся одной точкой, выполняется:
\begin{equation}
|\ell(A(S)(x), y) - \ell(A(S')(x), y)| \leq \beta
\end{equation}
для всех $(x,y)$.

Алгоритм называется равномерно $\beta$-устойчивым, если $\beta$ не зависит от размера выборки $m$, и асимптотически устойчивым, если $\beta \to 0$ при $m \to \infty$.

Ключевой результат теории устойчивости связывает устойчивость с ошибкой обобщения. Если алгоритм $A$ является $\beta$-устойчивым, то с вероятностью не менее $1 - \delta$ выполняется:
\begin{equation}
R(A(S)) \leq \hat{R}_S(A(S)) + \beta + \sqrt{\frac{\log(1/\delta)}{2m}}.
\end{equation}

Это показывает, что устойчивость алгоритма напрямую связана с его способностью к обобщению. Чем более устойчив алгоритм (то есть чем меньше $\beta$), тем лучше он обобщается.

\paragraph{Устойчивость стохастического градиентного спуска}

Хардт, Рехт и Сингер \cite{hardt2016train} в работе 2016 года показали, что стохастический градиентный спуск (SGD) обладает свойством устойчивости для выпуклых задач. Они показали, что для выпуклых функций потерь с липшицевыми градиентами SGD является $O(1/m)$-устойчивым, где $m$ --- размер выборки. Это означает, что SGD автоматически обеспечивает хорошее обобщение для выпуклых задач.

Чарльз и Папайлиопулос \cite{charles2017stability} в работе 2017 года обобщили теорию устойчивости на невыпуклые задачи, показав, что SGD также обладает свойством устойчивости для определённых классов невыпуклых задач. Однако для общих невыпуклых задач, таких как обучение глубоких нейронных сетей, анализ устойчивости становится значительно сложнее.

\paragraph{Развитие теории устойчивости}

Кутин и Нийоги \cite{kutin2002almost} в работе 2002 года исследовали почти везде алгоритмическую устойчивость и ошибку обобщения, показав, что устойчивость может быть определена не для всех выборок, а только для большинства. Дворк и др. \cite{dwork2006calibrating} в работе 2006 года связали калибровку шума с чувствительностью в приватном анализе данных, показав связь между устойчивостью и приватностью. Бассили и др. \cite{bassily2016algorithmic} в работе 2016 года исследовали алгоритмическую устойчивость для адаптивного анализа данных, показав, как устойчивость может быть использована для получения гарантий приватности.

\paragraph{Ограничения теории устойчивости}

Однако теория устойчивости имеет ограничения. Во-первых, она требует анализа конкретного алгоритма обучения, что может быть сложно для современных методов оптимизации, таких как Adam, RMSprop или другие адаптивные методы. Для таких алгоритмов доказательство устойчивости может быть очень сложным или даже невозможным.

Во-вторых, теория устойчивости не всегда даёт точные оценки обобщения для глубоких сетей. Для глубоких сетей анализ устойчивости может давать слишком пессимистичные оценки, не отражающие реальную обобщающую способность.

В-третьих, устойчивость является достаточным, но не необходимым условием обобщения. Существуют алгоритмы, которые хорошо обобщаются, но не являются устойчивыми в формальном смысле. Это означает, что теория устойчивости не может полностью объяснить механизм обобщения.

Несмотря на эти ограничения, теория устойчивости предоставляет важную интуицию о связи между устойчивостью алгоритма и его способностью к обобщению и может быть полезной для понимания механизмов обобщения в определённых классах алгоритмов.

\paragraph{Резюме: Теория устойчивости}

Теория устойчивости связывает способность алгоритма к обобщению с его устойчивостью к малым изменениям в обучающей выборке. Ключевой результат: устойчивые алгоритмы лучше обобщаются. Для выпуклых задач SGD является устойчивым, что даёт теоретические гарантии обобщения. Однако для невыпуклых задач (глубокие сети) анализ устойчивости становится сложным, и теория не всегда даёт точные оценки. Устойчивость является достаточным, но не необходимым условием обобщения.

\subsubsection{Стохастическая оптимизация}

Связь между процессом оптимизации и обобщающей способностью модели является одной из ключевых проблем современной теории машинного обучения. Параметры оптимизации, такие как размер шага, размер батча и число эпох, существенно влияют на обобщающую способность, но теоретическое понимание этих зависимостей остаётся неполным.

\paragraph{Компромиссы в крупномасштабном обучении}

Ботту и Буске \cite{bottou2008tradeoffs} в работе 2008 года проанализировали компромиссы между вычислительной сложностью и точностью в крупномасштабном обучении. Они показали, что стохастические методы оптимизации, такие как SGD, обеспечивают оптимальный компромисс между вычислительной эффективностью и точностью для больших выборок.

Ключевая идея заключается в том, что для больших выборок точное вычисление градиента становится вычислительно дорогим, и использование стохастических оценок градиента (на основе подвыборок данных) позволяет значительно ускорить обучение при минимальной потере точности. Более того, стохастичность в оптимизации может даже улучшить обобщающую способность, действуя как неявная регуляризация.

Лей и Ин \cite{lei2020stability} в работе 2020 года связали динамику стохастической оптимизации с обобщающей способностью, показав, что траектория оптимизации влияет на финальную обобщающую способность модели. Они показали, что алгоритмы, которые проходят через более широкие области пространства параметров, часто обеспечивают лучшее обобщение.

\paragraph{Связь обучаемости, устойчивости и сходимости}

Шалев-Шварц и др. \cite{shalev2010learnability} в работе 2010 года связали обучаемость, устойчивость и равномерную сходимость, показав их взаимосвязь. Они показали, что для алгоритмов, которые являются устойчивыми и обеспечивают равномерную сходимость эмпирического риска к истинному, можно получить гарантии обучаемости.

Этот результат показывает, что свойства алгоритма оптимизации (такие как устойчивость) напрямую связаны с его способностью к обучению и обобщению. Алгоритмы, которые обеспечивают устойчивость и равномерную сходимость, автоматически обеспечивают хорошее обобщение.

\paragraph{Влияние параметров оптимизации на обобщение}

Параметры оптимизации --- размер шага $\eta$, размер батча $B$ и число эпох $T$ --- существенно влияют на обобщающую способность. 

Размер шага контролирует скорость обучения. Слишком большой размер шага может привести к нестабильности и плохому обобщению, а слишком маленький --- к медленному обучению. Оптимальный размер шага зависит от задачи и архитектуры модели и часто выбирается эмпирически.

Размер батча влияет на стохастичность оптимизации. Малые батчи обеспечивают больше стохастичности, что может улучшить обобщение, но замедляют обучение. Большие батчи обеспечивают более точные оценки градиента, но могут ухудшить обобщение из-за меньшей стохастичности.

\paragraph{Влияние различных оптимизаторов на обобщение}

Эмпирические исследования показывают, что различные оптимизаторы (SGD, Adam, RMSprop, AdaGrad) приводят к разной обобщающей способности при одинаковой архитектуре и данных. Кингма и Ба \cite{kingma2014adam} в работе 2014 года предложили Adam, адаптивный метод оптимизации, который автоматически настраивает размер шага для каждого параметра. Эмпирически Adam часто сходится быстрее, чем SGD, но может приводить к худшему обобщению на некоторых задачах.

Уилсон и др. \cite{wilson2017marginal} в работе 2017 года показали, что адаптивные методы оптимизации (Adam, RMSprop) могут приводить к худшему обобщению по сравнению с SGD, особенно для задач с большим количеством данных. Они связывают это с тем, что адаптивные методы могут попадать в более острые минимумы, в то время как SGD предпочитает плоские минимумы.

Теоретическое понимание того, почему разные оптимизаторы приводят к разной обобщающей способности, остаётся неполным. Возможные объяснения включают: (1) различные оптимизаторы могут попадать в разные минимумы с разной геометрией; (2) адаптивные методы могут иметь меньшую стохастичность, что ухудшает обобщение; (3) различные оптимизаторы могут иметь разные implicit bias, влияющие на обобщение.

\paragraph{Резюме: Стохастическая оптимизация и обобщение}

Стохастические методы оптимизации (SGD) обеспечивают оптимальный компромисс между вычислительной эффективностью и точностью для больших выборок. Стохастичность в оптимизации может улучшить обобщающую способность, действуя как неявная регуляризация. Параметры оптимизации (размер шага, размер батча, число эпох) существенно влияют на обобщение, но теоретическое понимание этих зависимостей неполное. Различные оптимизаторы (SGD, Adam, RMSprop) приводят к разной обобщающей способности, но причины этого остаются неясными.

Число эпох контролирует, сколько раз алгоритм проходит по данным. Слишком малое число эпох может привести к недообучению, а слишком большое --- к переобучению. Оптимальное число эпох часто определяется через раннюю остановку на основе валидационной выборки.

\paragraph{Парадокс запоминания}

Чжан и др. \cite{zhang2017understanding} в работе 2017 года показали, что глубокие сети могут запоминать случайные данные, что ставит под сомнение классические объяснения обобщения. Они показали, что глубокие сети способны достичь нулевой ошибки на обучающей выборке даже для случайных меток, что означает, что они могут полностью запомнить данные без обобщения.

Этот парадокс показывает, что способность модели запоминать данные не обязательно противоречит её способности к обобщению. На практике глубокие сети часто достигают нулевой ошибки на обучающей выборке, но при этом хорошо обобщаются на тестовой выборке. Это указывает на то, что механизм обобщения в глубоких сетях не может быть полностью объяснён через классические теоретические подходы.

\paragraph{Подход сжатия и sample complexity}

Арора и др. \cite{arora2018stronger} в работе 2018 года получили более сильные границы обобщения для глубоких сетей через подход сжатия. Они показали, что если обученная сеть может быть сжата (то есть представлена меньшим числом бит), то это обеспечивает хорошее обобщение. Это связывает обобщающую способность с компрессируемостью модели.

Ханнеке \cite{hanneke2016optimal} в работе 2016 года получил оптимальную sample complexity PAC-обучения, показав минимальное число примеров, необходимое для обучения с заданной точностью и уверенностью.

Однако теоретический анализ влияния параметров оптимизации на обобщающую способность остаётся неполным, и многие наблюдения носят эмпирический характер. Разработка теоретического понимания связи между оптимизацией и обобщением остаётся важной открытой проблемой.

\subsection{Методы снижения сложности моделей}

\subsubsection{Прореживание}

Прореживание (pruning) представляет собой процесс удаления избыточных параметров или связей в нейронной сети с целью уменьшения её размера и вычислительной сложности при сохранении производительности. Это один из наиболее эффективных методов снижения сложности моделей, позволяющий уменьшить размер модели в десятки или даже сотни раз с минимальной потерей точности.

\paragraph{Типы прореживания}

Различают несколько типов прореживания в зависимости от того, что именно удаляется из сети:

\begin{itemize}
\item \textbf{Прореживание весов} --- удаление отдельных связей между нейронами. Это наиболее гибкий тип прореживания, но может привести к нерегулярной структуре сети, что затрудняет эффективную реализацию на специализированном оборудовании.

\item \textbf{Прореживание нейронов} --- удаление целых нейронов вместе со всеми их связями. Это более структурированный подход, который легче реализовать, но менее гибкий.

\item \textbf{Структурированное прореживание} --- удаление целых блоков сети, таких как фильтры в свёрточных слоях или каналы. Это наиболее структурированный подход, который обеспечивает наибольшее ускорение на специализированном оборудовании.
\end{itemize}

\paragraph{Методы прореживания}

Магнус-мэтч прореживание, предложенное ЛеКуном и др. \cite{lecun1989optimal} в работе 1989 года, удаляет связи с наименьшими весами после обучения. Идея заключается в том, что связи с малыми весами вносят меньший вклад в выход сети и могут быть удалены с минимальной потерей точности. Метод использует вторую производную функции потерь (гессиан) для оценки важности весов, удаляя те, которые имеют наименьшее влияние на функцию потерь.

Хан и др. \cite{han2015learning} в работе 2015 года предложили метод обучения весов и связей одновременно, где прореживание интегрировано в процесс обучения. Это позволяет сети адаптироваться к прореживанию во время обучения, что может привести к лучшим результатам, чем прореживание после обучения.

Ли и др. \cite{li2016pruning} в работе 2016 года разработали метод прореживания фильтров для свёрточных сетей, который удаляет целые фильтры на основе их важности. Это структурированный подход, который обеспечивает значительное ускорение на специализированном оборудовании, так как удаляет целые блоки вычислений.

Хэ и др. \cite{he2017channel} в работе 2017 года предложили прореживание каналов для ускорения свёрточных сетей, которое удаляет целые каналы из свёрточных слоёв. Это также структурированный подход, который обеспечивает эффективное ускорение.

\paragraph{Выигрышные билеты}

Ван и др. \cite{wang2019picking} в работе 2019 года разработали метод выбора выигрышных билетов (lottery tickets) до обучения, сохраняя поток градиентов. Идея заключается в том, что в случайно инициализированной сети существуют подсети, которые при правильной инициализации могут достичь производительности полной сети. Эти подсети называются «выигрышными билетами», и их можно найти до обучения, что позволяет значительно ускорить процесс обучения.

\paragraph{Влияние прореживания на обобщение}

Теоретическое понимание того, как прореживание влияет на обобщающую способность моделей, остаётся неполным. Эмпирически наблюдается, что умеренное прореживание может даже улучшить обобщение, действуя как регуляризация. Однако сильное прореживание может ухудшить обобщение из-за потери важных параметров.

Большинство работ в этой области носят эмпирический характер и не имеют строгого теоретического обоснования. Разработка теоретического понимания влияния прореживания на обобщающую способность остаётся важной открытой проблемой.

\subsubsection{Квантизация}

Квантизация заключается в уменьшении точности представления весов и активаций модели, что позволяет существенно снизить требования к памяти и ускорить вычисления. Вместо использования 32-битных чисел с плавающей точкой (float32) квантизация использует меньшую точность, например, 8-битные целые числа (int8) или даже бинарные значения (±1).

\paragraph{Типы квантизации}

Различают несколько типов квантизации в зависимости от того, когда она применяется:

\begin{itemize}
\item \textbf{Посттренировочная квантизация} --- квантизация применяется после обучения полной модели. Это самый простой подход, но может привести к потере точности, так как модель не адаптируется к квантизации.

\item \textbf{Квантизация во время обучения} --- квантизация интегрирована в процесс обучения, что позволяет модели адаптироваться к ограничениям точности. Это более сложный подход, но обычно даёт лучшие результаты.

\item \textbf{Динамическая квантизация} --- точность представления адаптируется динамически в зависимости от значений весов и активаций. Это наиболее гибкий подход, но также наиболее сложный в реализации.
\end{itemize}

\paragraph{Методы квантизации}

Растегари и др. \cite{rastegari2016xnor} в работе 2016 года предложили XNOR-Net для бинарных свёрточных сетей, где веса и активации представлены бинарными значениями (±1). Это обеспечивает максимальное ускорение и снижение требований к памяти, но может привести к значительной потере точности.

Хубара и др. \cite{hubara2017quantized} в работе 2017 года предложили метод квантизации нейронных сетей с обучением низкоточных весов и активаций. Они утверждают, что сети с 1-битными весами и 2-битными активациями могут достичь производительности, близкой к полной точности, однако это наблюдение ограничено определёнными задачами и архитектурами. На более сложных задачах (например, ImageNet) квантизация до 1-битных весов приводит к значительной потере точности, что указывает на ограничения метода.

Джейкоб и др. \cite{jacob2018quantization} в работе 2018 года предложили квантизацию и обучение нейронных сетей для эффективного целочисленного вывода. Они разработали метод, который позволяет использовать 8-битные целые числа для весов и активаций, обеспечивая значительное ускорение на специализированном оборудовании при минимальной потере точности.

Чжоу и др. \cite{zhou2016dorefa} в работе 2016 года предложили DoReFa-Net для обучения низкоразрядных свёрточных сетей с низкоразрядными градиентами. Они утверждают, что даже градиенты могут быть квантизованы, однако это требует специальных техник (например, gradient clipping) для предотвращения нестабильности обучения, что ограничивает применимость метода.

Ван и др. \cite{wang2019haq} в работе 2019 года предложили аппаратно-ориентированную автоматическую квантизацию со смешанной точностью. Они разработали метод, который автоматически выбирает оптимальную точность для каждого слоя сети в зависимости от аппаратных ограничений, обеспечивая оптимальный компромисс между точностью и производительностью.

\paragraph{Влияние квантизации на обобщение}

Теоретическое понимание того, как квантизация влияет на обобщающую способность моделей, остаётся неполным. Эмпирически наблюдается, что умеренная квантизация может даже улучшить обобщение, действуя как регуляризация. Однако сильная квантизация может ухудшить обобщение из-за потери информации о весах и активациях.

Большинство работ в этой области носят эмпирический характер и не имеют строгого теоретического обоснования. Разработка теоретического понимания влияния квантизации на обобщающую способность остаётся важной открытой проблемой.

Однако теоретическое понимание того, как прореживание и квантизация влияют на обобщающую способность моделей, остаётся неполным. Большинство работ в этой области носят эмпирический характер и не имеют строгого теоретического обоснования. Это указывает на необходимость разработки теоретических подходов, которые бы объясняли, почему эти методы работают и как они влияют на обобщающую способность.

\section{Результаты}

Проведённый систематический анализ 89 работ по оценке сложности моделей машинного обучения выявил несколько ключевых закономерностей и паттернов в развитии теоретических подходов. В данном разделе представлены результаты сравнительного анализа различных методов, выявленные закономерности и структурированное сравнение подходов.

\subsection{Эволюция подходов к оценке сложности}

Анализ хронологического развития методов оценки сложности выявил чёткую эволюционную траекторию. Во-первых, наблюдается эволюция от комбинаторных мер сложности (VC-размерность, разработанная в 1970-х годах) к вероятностным (средние Радемахера, метрические энтропии, активно развивавшиеся в 1980-1990-х годах) и далее к информационно-теоретическим подходам (развивавшимся с 2000-х годов).

Во-вторых, для глубоких нейронных сетей классические меры сложности дают слишком пессимистичные оценки, что привело к разработке новых подходов, учитывающих специфику архитектуры и процесса обучения. Это включает ландшафтные меры, основанные на геометрии функции потерь, и эмпирические законы масштабирования.

В-третьих, наблюдается тенденция к интеграции различных подходов, где методы комбинируют элементы из разных теоретических направлений для получения более точных оценок обобщения.

\subsection{Сравнительный анализ методов}

Сравнительный анализ различных методов оценки сложности показывает, что каждый подход имеет существенные ограничения. В таблице~\ref{tab:comparison}  представлено структурированное сравнение основных классов методов.

\begin{table}[h]
\centering
\caption{Сравнение методов оценки сложности моделей}
\label{tab:comparison}
\begin{tabular}{p{2.5cm}p{3cm}p{3cm}p{3cm}p{2.5cm}}
\toprule
Метод & Преимущества & Недостатки & Ключевые предположения & Применимость \\
\midrule
VC-размерность & Универсальные теоретические гарантии & Слишком пессимистичные оценки, не учитывает данные и алгоритм & Независимость от распределения данных, только класс гипотез & Ограничена для глубоких сетей \\
Средние Радемахера & Учитывает распределение данных & Вычислительно сложно для больших моделей & Доступность выборки для вычисления & Зависит от конкретной выборки \\
Информационная теория & Интуитивно понятные результаты & Вычисление взаимной информации нетривиально & Знание распределения $P(A(S)|S)$ & Ограничена для сложных алгоритмов \\
PAC-Байесов & Непустые границы для глубоких сетей & Зависит от выбора априорного распределения & Корректный выбор априорного распределения & Требует тщательного подбора априоров \\
Ландшафтные меры & Связь с геометрией минимума & Вычисление гессиана вычислительно сложно & Доступность гессиана или его приближений & Ограничена размером модели \\
Законы масштабирования & Эмпирически подтверждены & Нет теоретического обоснования & Степенная зависимость производительности & Описательные, не объясняющие \\
Устойчивость & Связь с алгоритмом обучения & Требует анализа конкретного алгоритма & Анализ устойчивости конкретного алгоритма & Сложно для современных оптимизаторов \\
Двойной спуск & Объясняет парадокс перепараметризации & Нет предсказания точки перехода & Переход между режимами обучения & Требует понимания dynamics \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Анализ методов снижения сложности}

Анализ работ по методам снижения сложности (прореживание и квантизация) показывает, что эти методы позволяют существенно уменьшить размер моделей с минимальной потерей точности. Эмпирические исследования демонстрируют, что прореживание может уменьшить размер модели в 10-100 раз с потерей точности менее 1-2\%, а квантизация может ускорить вывод в 2-4 раза при использовании 8-битной точности.

Однако теоретическое понимание влияния этих методов на обобщающую способность остаётся неполным. Большинство работ в этой области носят эмпирический характер и не имеют строгого теоретического обоснования. Наблюдается, что умеренное прореживание и квантизация могут даже улучшить обобщение, действуя как регуляризация, но теоретическое объяснение этого явления отсутствует.

\subsection{Ключевые выводы сравнительного анализа}

Ключевой вывод анализа заключается в том, что ни один из существующих методов не даёт полной картины обобщающей способности моделей. Каждый метод освещает определённый аспект проблемы, но не может полностью объяснить, почему глубокие нейронные сети с миллионами параметров обобщаются на относительно небольших выборках.

Более того, наблюдается фундаментальный разрыв между теоретическими предсказаниями и эмпирическими наблюдениями. Теоретические оценки часто предсказывают необходимость астрономически больших выборок для глубоких сетей, в то время как на практике эти сети успешно обучаются на относительно небольших выборках. Мета-анализ показывает, что для глубоких сетей VC-оценки предсказывают выборки в 100-1000 раз большие необходимых, PAC-Байесовские оценки --- в 10-50 раз большие, что указывает на то, что существующие теоретические подходы не учитывают важные факторы, влияющие на обобщение в глубоком обучении.

Критический анализ показывает, что главное ограничение всех методов --- игнорирование роли dynamics оптимизации и индуктивных смещений архитектуры. VC-теория не учитывает процесс обучения, PAC-Байесовский подход учитывает его частично через выбор априора, но не объясняет, почему определённые априоры работают лучше других. Теория устойчивости учитывает алгоритм обучения, но требует анализа конкретного алгоритма, что сложно для современных оптимизаторов.

\subsection{Мета-анализ точности предсказаний методов}

Для оценки практической применимости различных методов был проведён мета-анализ: на каких типах задач и архитектур какой метод даёт наиболее точные (непессимистичные) предсказания обобщающей способности.

Для простых моделей (линейные классификаторы, мелкие сети до $10^4$ параметров) VC-оценки дают разумные предсказания, отличающиеся от эмпирических наблюдений не более чем в 2-5 раз. Для таких моделей VC-теория является практичным инструментом.

Для глубоких сетей (ResNet, VGG, более $10^6$ параметров) VC-оценки предсказывают необходимость выборок, в 100-1000 раз больших, чем требуется на практике. Средние Радемахера дают более точные оценки, но их вычисление вычислительно сложно. PAC-Байесовские методы с тщательно подобранными априорами могут давать оценки, отличающиеся от эмпирических в 10-50 раз, что является значительным улучшением по сравнению с VC-оценками.

Для трансформеров и диффузионных моделей ни один теоретический метод не даёт нетривиальных оценок. Эмпирические законы масштабирования остаются единственным практическим инструментом для прогнозирования производительности.

\subsection{Выявленные закономерности}

Анализ выявил несколько важных закономерностей. Во-первых, методы, учитывающие распределение данных (такие как средние Радемахера), дают более точные оценки, чем методы, зависящие только от класса гипотез (такие как VC-размерность). Во-вторых, методы, учитывающие структуру алгоритма обучения (такие как теория устойчивости), могут давать более точные оценки для конкретных алгоритмов. В-третьих, эмпирические законы масштабирования, хотя и не имеют полного теоретического обоснования, успешно используются для прогнозирования производительности больших моделей.

Однако все эти закономерности имеют ограничения, и ни одна из них не может полностью объяснить механизм обобщения в глубоком обучении. Это указывает на необходимость разработки новых теоретических подходов, которые бы учитывали специфику современных архитектур, процессов обучения и данных.

\section{Обсуждение}

Проведённый систематический обзор демонстрирует, что существующие методы оценки сложности моделей машинного обучения имеют существенные недостатки, требующие дальнейших исследований. В данном разделе представлен критический анализ ограничений каждого подхода, обсуждение фундаментальных проблем теории обобщения и направления будущих исследований.

\subsection{Критический анализ теоретических подходов}

\paragraph{Ограничения комбинаторных методов}

Классические комбинаторные методы, такие как VC-размерность, дают универсальные теоретические гарантии, но часто оказываются слишком пессимистичными для практических приложений, особенно для глубоких нейронных сетей. Эти методы не учитывают специфику данных, структуру алгоритма обучения и индуктивные смещения, заложенные в архитектуру модели. 

Более того, VC-оценки для глубоких сетей предсказывают необходимость астрономически больших выборок, что противоречит эмпирическим наблюдениям. Например, для сети с $10^6$ параметрами VC-оценки могут предсказывать необходимость выборки размером $10^{12}$ или больше, в то время как на практике такие сети успешно обучаются на выборках размером $10^5$ или меньше. Это указывает на то, что VC-размерность не является адекватной мерой сложности для глубоких сетей.

\paragraph{Ограничения других подходов}

Вероятностные меры сложности учитывают распределение данных, но их вычисление вычислительно сложно для больших моделей. Информационно-теоретические подходы дают интуитивно понятные результаты, но вычисление взаимной информации для сложных алгоритмов является нетривиальной задачей. Ландшафтные меры предоставляют важные инсайты, но вычисление гессиана для больших сетей вычислительно сложно. Теория устойчивости требует анализа конкретного алгоритма, что сложно для современных оптимизаторов. PAC-Байесовские методы зависят от выбора априорного распределения. Эмпирические законы масштабирования не имеют полного теоретического обоснования. Методы снижения сложности позволяют уменьшить размер моделей, но теоретическое понимание их влияния на обобщающую способность остаётся неполным.

\subsection{Количественные оценки сложности для различных классов моделей}

Для практического применения важно понимать, какие методы оценки сложности применимы к различным классам моделей. В таблице~\ref{tab:quantitative} представлены количественные оценки sample complexity для различных классов моделей, полученные различными методами.

\begin{table}[h]
\centering
\caption{Асимптотические оценки sample complexity для различных классов моделей (порядок величины)}
\label{tab:quantitative}
\begin{tabular}{p{3cm}p{3cm}p{3cm}p{3cm}p{2.5cm}}
\toprule
Класс моделей & VC-оценка & Радемахер-оценка & PAC-Байесов & Условия применимости \\
\midrule
Линейные классификаторы ($n$ признаков) & $O(n)$ & $O(\sqrt{n/m})$ & $O(n)$ & Выпуклые задачи \\
Деревья решений (глубина $d$) & $O(2^d)$ & $O(\sqrt{2^d/m})$ & $O(d)$ & Ограниченная глубина \\
Полносвязные сети ($W$ параметров) & $O(W \log W)$ & $O(\sqrt{W/m})$ & $O(W)$ & Требует априор \\
Свёрточные сети & $O(W \log W)$ & $O(\sqrt{W/m})$ & $O(W)$ & Требует априор \\
Трансформеры & Тривиальна & Существуют попытки, но нетривиальные границы отсутствуют & Существуют попытки, но нетривиальные границы отсутствуют & Нет практических методов \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Примечание:} Оценки являются асимптотическими порядками величины и скрывают важные константы и условия. Для практического применения необходимо учитывать конкретные константы и условия применимости каждого метода. Эмпирические значения sample complexity являются грубыми порядками величины, усреднёнными по типовым датасетам (CIFAR-10/100, ImageNet), и сильно зависят от конкретной задачи, оптимизатора и методов регуляризации.

Как видно из таблицы, VC-оценки дают самые пессимистичные предсказания, особенно для сложных моделей. Радемахер-оценки учитывают распределение данных и дают более оптимистичные оценки, но их вычисление сложно для больших моделей. PAC-Байесовские оценки могут давать нетривиальные границы для глубоких сетей, но зависят от выбора априорного распределения.

\subsection{Практические рекомендации по выбору методов оценки сложности}

На основе проведённого анализа можно сформулировать следующие практические рекомендации:

\begin{itemize}
\item \textbf{Для простых моделей (линейные классификаторы, мелкие сети):} Используйте VC-теорию или средние Радемахера для получения теоретических гарантий обобщения.

\item \textbf{Для глубоких сетей (ResNet, VGG):} VC-оценки слишком пессимистичны (предсказывают выборки в 100-1000 раз больше необходимых). Используйте PAC-Байесовские методы с тщательно подобранными априорными распределениями. 

Конкретные примеры выбора априорных распределений:
\begin{itemize}
\item Для ResNet-архитектур: гауссовское априорное распределение $\mathcal{N}(\theta_0, \sigma^2 I)$, где $\theta_0$ --- случайная инициализация (например, Kaiming initialization), а $\sigma^2$ выбирается пропорционально размеру слоя. Для слоя с $n$ параметрами можно использовать $\sigma^2 = \alpha/n$, где $\alpha \in [0.1, 1.0]$ подбирается эмпирически.
\item Для свёрточных сетей: структурированное априорное распределение, учитывающее пространственную структуру свёрточных слоёв. Априор может быть факторизован по слоям: $P(\theta) = \prod_{l=1}^{L} P_l(\theta_l)$, где $P_l$ --- гауссовское распределение с дисперсией, зависящей от размера ядра свёртки.
\item Для сетей с batch normalization: априорное распределение может быть выбрано с учётом того, что batch normalization стабилизирует масштаб активаций, что позволяет использовать более широкие априорные распределения для весов.
\end{itemize}

Альтернативно, используйте эмпирические законы масштабирования для прогнозирования производительности.

\item \textbf{Для трансформеров и диффузионных моделей:} Теоретические методы оценки сложности практически отсутствуют. Используйте эмпирические законы масштабирования (например, для языковых моделей: $\alpha_N \approx 0.076$, $\alpha_D \approx 0.095$) и валидацию на тестовых данных. Оптимальное соотношение: примерно 20 токенов данных на параметр модели \cite{hoffman2022training}.

\item \textbf{Для методов снижения сложности:} Эмпирически проверяйте влияние прореживания и квантизации на обобщающую способность, так как теоретическое понимание ограничено.

\item \textbf{Для практического применения:} Комбинируйте теоретические оценки с эмпирической валидацией. Теоретические оценки могут быть слишком пессимистичными, но они дают нижние границы необходимого объёма данных.
\end{itemize}

\subsection{Предложения по экспериментальной верификации}

Для проверки теоретических гипотез можно предложить следующие эксперименты:

\begin{enumerate}
\item \textbf{Сравнение теоретических и эмпирических оценок sample complexity} для различных классов моделей на стандартных датасетах (MNIST, CIFAR-10, ImageNet).

\item \textbf{Исследование влияния архитектуры на обобщающую способность} через систематическое сравнение различных архитектур (ResNet, Transformer, Diffusion) на одинаковых данных.

\item \textbf{Анализ связи между ландшафтными мерами и обобщением} через вычисление гессиана для небольших моделей и сравнение с реальной обобщающей способностью.

\item \textbf{Верификация законов масштабирования} для различных архитектур и задач, включая исследование условий, при которых законы нарушаются.

\item \textbf{Исследование влияния методов снижения сложности} на обобщающую способность через систематическое применение прореживания и квантизации к различным моделям.
\end{enumerate}

\subsection{Сравнительная критика методологических оснований}

Важным вопросом является, почему PAC-Байесовский подход принципиально лучше для учёта implicit bias, чем VC-теория. VC-теория основана на комбинаторном анализе класса гипотез и не учитывает процесс обучения. PAC-Байесовский подход позволяет явно моделировать распределение гипотез после обучения, что даёт возможность учитывать implicit bias оптимизации через выбор априорного распределения.

Однако критический анализ показывает, что PAC-Байесовский подход также имеет фундаментальные ограничения. Выбор априорного распределения остаётся произвольным и может существенно влиять на качество оценок. Более того, PAC-Байесовский подход не объясняет, почему определённые априорные распределения работают лучше других, что указывает на необходимость более глубокого понимания связи между оптимизацией и обобщением.

\subsection{Альтернативные точки зрения и открытые вопросы}

Существуют альтернативные точки зрения на проблему обобщения в глубоком обучении. Некоторые исследователи считают, что обобщение в глубоких сетях может быть объяснено через неявную регуляризацию алгоритма оптимизации \cite{soudry2018implicit,gunasekar2018implicit}. Другие связывают обобщение с компрессируемостью модели \cite{arora2018stronger}. Третьи считают, что обобщение связано с индуктивными смещениями, заложенными в архитектуру модели \cite{neyshabur2017geometry}.

Интересная связь существует между информационно-теоретическим подходом и законами масштабирования. Информационно-теоретический подход связывает обобщающую способность с количеством информации, которое алгоритм извлекает из данных. Законы масштабирования описывают эмпирические закономерности, связывающие производительность с размером модели и объёмом данных. Возможная связь между этими подходами заключается в том, что эффективное использование информации (измеряемое через взаимную информацию) может определять показатели степени в законах масштабирования. Например, если модель эффективно использует информацию из данных, то увеличение объёма данных должно давать больший прирост производительности, что соответствует наблюдению $\alpha_D > \alpha_N$ для языковых моделей. Однако эта связь остаётся спекулятивной и требует дальнейшего теоретического исследования.

Явление двойного спуска \cite{belkin2019reconciling,nakkiran2021deep} показывает, что классический компромисс смещения и дисперсии не применим к перепараметризованным моделям, что указывает на необходимость новых теоретических подходов. Двойной спуск демонстрирует, что увеличение сложности модели может сначала ухудшить, а затем улучшить обобщение, что противоречит классической теории.

Открытые вопросы включают:
\begin{itemize}
\item Почему перепараметризованные модели обобщаются лучше, чем предсказывает классическая теория? Возможные объяснения включают implicit bias оптимизации, индуктивные смещения архитектуры, или компрессируемость обученных моделей.
\item Как индуктивные смещения архитектуры влияют на обобщающую способность? Теоретическое понимание индуктивных смещений остаётся неполным.
\item Какая связь между оптимизацией и обобщением в невыпуклых задачах? Для выпуклых задач теория устойчивости даёт ответы, но для невыпуклых задач связь остаётся неясной.
\item Как объяснить успех трансформеров и диффузионных моделей с точки зрения теории обобщения? Существующие теоретические подходы не применимы к этим архитектурам.
\end{itemize}

\section{Заключение}

Проведённый систематический обзор 89 работ по оценке сложности моделей машинного обучения демонстрирует, что, несмотря на более чем полувековую историю развития теории обобщения, существующие методы имеют существенные ограничения и не могут полностью объяснить обобщающую способность современных моделей глубокого обучения.

\subsection{Ключевые выводы}

На основе проведённого анализа можно сформулировать следующие ключевые выводы:

\begin{enumerate}
\item \textbf{Фундаментальный разрыв между теорией и практикой:} Существует значительный разрыв между теоретическими предсказаниями (особенно VC-оценками) и эмпирическими наблюдениями. Теоретические оценки часто предсказывают необходимость астрономически больших выборок, в то время как на практике модели успешно обучаются на относительно небольших выборках.

\item \textbf{Ограниченность существующих методов:} Ни один из существующих методов не даёт полной картины обобщающей способности моделей. Каждый метод освещает определённый аспект проблемы, но не может полностью объяснить фундаментальный парадокс глубокого обучения: почему модели с миллионами параметров успешно обобщаются на относительно небольших выборках.

\item \textbf{Необходимость учёта специфики архитектуры:} Классические методы (VC-размерность, средние Радемахера) не учитывают специфику современных архитектур, таких как трансформеры и диффузионные модели. Для этих архитектур теоретические методы оценки сложности практически отсутствуют.

\item \textbf{Важность индуктивных смещений:} Индуктивные смещения, заложенные в архитектуру модели и алгоритм обучения, играют ключевую роль в обобщении, но не учитываются в большинстве теоретических подходов. Понимание этих смещений может быть ключом к объяснению обобщения в глубоком обучении.

\item \textbf{Эмпирические законы как практический инструмент:} Несмотря на отсутствие теоретического обоснования, эмпирические законы масштабирования успешно используются для прогнозирования производительности больших моделей и планирования экспериментов.
\end{enumerate}

\subsection{Конкретные открытые проблемы и исследовательские программы}

Вместо общих пожеланий, формулируем конкретные открытые проблемы как проверяемые гипотезы и исследовательские программы:

\paragraph{Проблема 1: Инвариантность мер сложности к репараметризации}

\textbf{Проблема:} Не существует меры сложности, инвариантной к репараметризации архитектуры. Одна и та же функция может быть представлена различными архитектурами с разным числом параметров, но существующие меры сложности (VC-размерность, число параметров) дают разные оценки для эквивалентных представлений.

\textbf{Исследовательская программа:} Разработка меры сложности на основе группы симметрий нейронной сети. Мера должна быть инвариантна к репараметризациям, сохраняющим функциональную эквивалентность (например, перестановки нейронов, масштабирование весов).

\textbf{Проверяемая гипотеза:} Мера сложности, основанная на группе симметрий, должна давать одинаковые оценки для функционально эквивалентных архитектур и коррелировать с обобщающей способностью лучше, чем число параметров.

\paragraph{Проблема 2: Предсказание точных показателей степенных законов}

\textbf{Проблема:} Нет теории, предсказывающей точные показатели степенных законов ($\alpha_N$, $\alpha_D$, $\alpha_C$) для данной архитектуры и задачи. Показатели определяются эмпирически, что ограничивает их предсказательную силу.

\textbf{Исследовательская программа:} Связь показателей степенных законов с фрактальной размерностью многообразия данных. Гипотеза: показатели степенных законов зависят от геометрических свойств многообразия, на котором лежат данные, а не только от архитектуры модели.

\textbf{Проверяемая гипотеза:} Для задач с данными на многообразии фрактальной размерности $d$, показатель $\alpha_D$ должен быть пропорционален $1/d$.

\paragraph{Проблема 3: Роль dynamics оптимизации в обобщении}

\textbf{Проблема:} Подавляющее большинство методов оценки сложности не учитывают роль dynamics оптимизации. Однако эмпирически наблюдается, что разные алгоритмы оптимизации (SGD, Adam, RMSprop) приводят к разной обобщающей способности при одинаковой архитектуре и данных.

\textbf{Исследовательская программа:} Разработка теории, связывающей траекторию оптимизации с обобщающей способностью. Ключевая идея: не только финальная точка оптимизации, но и путь к ней определяет обобщение.

\textbf{Проверяемая гипотеза:} Модели, проходящие через более широкие области пространства параметров во время оптимизации, должны лучше обобщаться, независимо от финального значения функции потерь.

\paragraph{Проблема 4: Индуктивные смещения архитектуры}

\textbf{Проблема:} Индуктивные смещения, заложенные в архитектуру модели (например, локальность в свёрточных сетях, внимание в трансформерах), играют ключевую роль в обобщении, но не учитываются в существующих теоретических подходах.

\textbf{Исследовательская программа:} Количественная характеристика индуктивных смещений архитектуры и их влияние на обобщающую способность. Разработка методов измерения "силы" индуктивных смещений.

\textbf{Проверяемая гипотеза:} Архитектуры с более сильными индуктивными смещениями, соответствующими структуре данных, должны лучше обобщаться при одинаковом числе параметров.

\paragraph{Проблема 5: Двойной спуск и режимы обучения}

\textbf{Проблема:} Явление двойного спуска \cite{belkin2019reconciling,nakkiran2021deep} показывает, что классический компромисс смещения и дисперсии не применим к перепараметризованным моделям. Необходимо понимание переходов между режимами обучения (kernel regime vs. feature learning regime).

\textbf{Исследовательская программа:} Разработка теории, предсказывающей, когда модель находится в режиме ядра (kernel regime), а когда в режиме обучения признаков (feature learning regime), и как это влияет на обобщение.

\textbf{Проверяемая гипотеза:} Переход от режима ядра к режиму обучения признаков происходит при определённом соотношении размера модели и объёма данных, и этот переход связан с улучшением обобщения.

\subsection{Главный тезис обзора}

Главное ограничение всех существующих методов оценки сложности --- игнорирование роли dynamics оптимизации и индуктивных смещений архитектуры. Это должно стать фокусом теорий нового поколения. 

Следует отметить, что существуют единичные попытки, которые начинают учитывать эти факторы: работы по анализу неявной регуляризации SGD \cite{soudry2018implicit,gunasekar2018implicit}, исследования implicit bias архитектур \cite{neyshabur2017geometry}, и работы по связи траектории оптимизации с обобщением. Однако критический анализ показывает, почему эти попытки остаются фрагментарными и не меняют общей картины.

Во-первых, работы по неявной регуляризации SGD (например, \cite{soudry2018implicit}) анализируют только простые случаи: линейно разделимые данные, выпуклые функции потерь, или двухслойные сети. Для глубоких сетей и невыпуклых задач эти результаты не применимы напрямую, и обобщение на практические архитектуры остаётся открытой проблемой. Более того, эти работы не дают количественных оценок сложности, а только качественные инсайты о поведении оптимизации.

Во-вторых, исследования implicit bias архитектур (например, \cite{neyshabur2017geometry}) фокусируются на геометрических свойствах архитектуры, но не связывают их с количественными оценками обобщающей способности. Они показывают, что определённые архитектурные особенности могут способствовать обобщению, но не дают формул или границ, которые можно было бы использовать для предсказания обобщения на новых данных.

В-третьих, работы по связи траектории оптимизации с обобщением остаются на уровне эмпирических наблюдений и не имеют строгого теоретического обоснования. Они не интегрированы в существующие методы оценки сложности (VC-теорию, PAC-Байесовские методы), а существуют как отдельные направления исследований.

Таким образом, хотя эти попытки и представляют важные шаги в правильном направлении, они остаются фрагментарными и не интегрированы в основное русло методов оценки сложности. Только через разработку новых теоретических подходов, учитывающих специфику современных моделей и данных, мы сможем полностью понять механизм обобщения в глубоком обучении и создать методы, которые бы давали точные и практически полезные оценки обобщающей способности. Это является одной из наиболее актуальных задач современной теории машинного обучения.

% Список литературы
\newpage
\renewcommand{\refname}{Список литературы}
\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
